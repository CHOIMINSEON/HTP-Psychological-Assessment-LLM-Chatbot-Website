"""
BLIP, InstructBLIP, CLIP Interrogatorë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ìº¡ì…”ë‹ ë¹„êµ
í•„ìš”í•œ íŒ¨í‚¤ì§€: transformers, pillow, accelerate, torch, torchvision, clip-interrogator
"""

from transformers import BlipProcessor, BlipForConditionalGeneration
from transformers import InstructBlipProcessor, InstructBlipForConditionalGeneration
from PIL import Image
import torch


def setup_device():
    """GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"âœ… Using device: {device}")
    return device


def generate_blip_caption(image, device):
    """BLIP ëª¨ë¸ë¡œ ìº¡ì…˜ ìƒì„±"""
    print("ğŸ”¹ Running BLIP...")
    processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-large")
    model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-large").to(device)
    
    inputs = processor(image, return_tensors="pt").to(device)
    out = model.generate(**inputs, max_length=100)
    caption = processor.decode(out[0], skip_special_tokens=True)
    
    return caption


def generate_instructblip_caption(image, device, prompt=None):
    """InstructBLIP ëª¨ë¸ë¡œ ìº¡ì…˜ ìƒì„±"""
    print("ğŸ”¹ Running InstructBLIP...")
    processor = InstructBlipProcessor.from_pretrained("Salesforce/instructblip-flan-t5-xl")
    model = InstructBlipForConditionalGeneration.from_pretrained("Salesforce/instructblip-flan-t5-xl").to(device)
    
    if prompt is None:
        prompt = "Describe this image in detail, including objects, colors, positions, sizes, shapes, and atmosphere."
    
    inputs = processor(images=image, text=prompt, return_tensors="pt").to(device)
    out = model.generate(**inputs, max_new_tokens=100)
    caption = processor.tokenizer.decode(out[0], skip_special_tokens=True)
    
    return caption


def generate_clip_interrogator_caption(image):
    """CLIP Interrogatorë¡œ ìº¡ì…˜ ìƒì„±"""
    try:
        print("ğŸ”¹ Running CLIP Interrogator...")
        from clip_interrogator import Config, Interrogator
        import open_clip
        
        ci = Interrogator(Config(clip_model_name="ViT-L-14/openai"))
        caption = ci.interrogate(image)
        return caption
    except Exception as e:
        return f"âš ï¸ CLIP Interrogator not run: {e}"


def main(image_path):
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = setup_device()
    
    # ì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸°
    try:
        image = Image.open(image_path).convert("RGB")
        print(f"âœ… ì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ: {image_path}")
    except FileNotFoundError:
        print(f"âŒ ì˜¤ë¥˜: ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {image_path}")
        return
    
    # ê²°ê³¼ ì €ì¥ìš© ë”•ì…”ë„ˆë¦¬
    results = {}
    
    # BLIP ìº¡ì…˜
    results["BLIP"] = generate_blip_caption(image, device)
    
    # InstructBLIP ìº¡ì…˜
    results["InstructBLIP"] = generate_instructblip_caption(image, device)
    
    # CLIP Interrogator ìº¡ì…˜
    results["CLIP Interrogator"] = generate_clip_interrogator_caption(image)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 50)
    print("ğŸ“¸ Detailed Caption Comparison")
    print("=" * 50)
    for model_name, caption in results.items():
        print(f"\nğŸ”¸ {model_name}:")
        print(caption)
    
    return results


if __name__ == "__main__":
    # ì´ë¯¸ì§€ ê²½ë¡œ ì„¤ì • (ì‚¬ìš©ìê°€ ìˆ˜ì •í•´ì•¼ í•¨)
    IMAGE_PATH = "/content/drive/MyDrive/Colab/T_V_T/htp/test_ë‚˜ë¬´.JPG"
    
    main(IMAGE_PATH)
