# rag_engine.py
import torch
import json
from typing import List, Dict
from pydantic import BaseModel, Field
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.output_parsers import JsonOutputParser
from langchain_core.documents import Document
from embeddings import vectorstore, cross_encoder


# ===============================================================
# 1) Multi Query Generator (OpenAI GPT-4o-mini)
# ===============================================================

class MultiQueryGenerator(BaseModel):
    queries: List[str] = Field(description="ê²€ìƒ‰ ì¿¼ë¦¬ ëª©ë¡")


class AdvancedQueryRewriter:
    def __init__(self, model_name="gpt-4o", temperature=0):
        self.llm = ChatOpenAI(
            model_name=model_name,
            temperature=temperature,
            max_tokens=1000
        )
        self.parser = JsonOutputParser(pydantic_object=MultiQueryGenerator)

        self.template = """
        ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰ìš© ì¿¼ë¦¬ë¥¼ ì¬ì‘ì„±í•©ë‹ˆë‹¤.

        # ì§€ì¹¨
        1. ì•„ë˜ì˜ history ì— í¬í•¨ëœ ëª¨ë“  ì´ì „ ì§ˆì˜/ê²€ìƒ‰ë¬¸ì„œ/ë‹µë³€ì„ ì°¸ê³ í•˜ì—¬ ë” ì •í™•í•œ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì„¸ìš”.
        2. í˜„ì¬ ì§ˆë¬¸ì´ ëª¨í˜¸í•˜ê±°ë‚˜ ìƒëµëœ ê²½ìš°, history ë¥¼ ì°¸ê³ í•´ ë¬¸ë§¥ìƒ ì™„ì „í•œ ì¿¼ë¦¬ë¡œ ì¬êµ¬ì„±í•˜ì„¸ìš”.
        3. ì´ì „ ëŒ€í™”ê°€ ì—†ê±°ë‚˜ ê´€ë ¨ì´ ì—†ëŠ” ê²½ìš° í˜„ì¬ ì§ˆë¬¸ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
        4. ë°˜ë“œì‹œ ëª…í™•í•˜ê³  ê²€ìƒ‰ì— ì í•©í•œ ì¿¼ë¥¼ ìƒì„±í•˜ì„¸ìš”.
        6. ì¶œë ¥ì€ ì¬ìƒì„±ëœ ì¿¼ë¦¬ ë¬¸ìì—´ë§Œ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤. ì¶”ê°€ ì„¤ëª…ì´ë‚˜ ì£¼ì„ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
        7. í˜„ì¬ ë¬¸ì¥ì´ ì—¬ëŸ¬ ì†ì„±ì„ í¬í•¨í•˜ê³  ìˆë‹¤ë©´, ê°ê°ì„ ë³„ë„ì˜ ì¿¼ë¦¬ë¡œ ë¶„ë¦¬í•˜ì„¸ìš”.
        8. ê° ì¿¼ë¦¬ëŠ” ë…ë¦½ì ìœ¼ë¡œ ë²¡í„° DBì—ì„œ ê²€ìƒ‰ë  ìˆ˜ ìˆë„ë¡ ì™„ì „í•˜ê³  ëª…í™•í•´ì•¼ í•©ë‹ˆë‹¤.
        10. ë¶„ë¦¬ëœ ì¿¼ë¦¬ë“¤ì„ í•©ì³¤ì„ ë•Œ ì›ë˜ ì¿¼ë¦¬ì˜ ì˜ë¯¸ë¥¼ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë„ë¡ í•˜ì„¸ìš”.

        ì „ì²´ ëŒ€í™”: {history_text}
        ì‚¬ìš©ì ì§ˆë¬¸: {current_query}

        ì¶œë ¥ ì˜ˆì‹œ:
        {
            "queries": ["ì¿¼ë¦¬1", "ì¿¼ë¦¬2"]
        }

        ì˜ˆì‹œ :
        í˜„ì¬ ì§ˆë¬¸ì´ "ì„œìš¸ì€ ? ê·¸ë¦¬ê³  ë§›ì§‘ì€?"ì´ê³  ì´ì „ ëŒ€í™”ê°€ "í•œêµ­ì˜ ê´€ê´‘ì§€ ì¶”ì²œí•´ì¤˜"ë¼ë©´,
        {{
            "queries" : ["ì„œìš¸ì˜ ê´€ê´‘ì§€ ì¶”ì²œí•´ì¤˜", "ì„œìš¸ì˜ ë§›ì§‘ ì¶”ì²œí•´ì¤˜"]
        }}

        ë‹¨ì¼ ì¿¼ë¦¬ì¸ ê²½ìš°:
        {{
            "queries" : ["í•œêµ­ì˜ ê´€ê´‘ì§€ ì¶”ì²œí•´ì¤˜"]
        }}

        {format_instructions}
        """

        self.prompt = PromptTemplate(
            input_variables=["history_text", "current_query"],
            template=self.template,
        )

    def rewrite_query(self, history_text: str, current_query: str) -> List[str]:
        if not history_text.strip():
            history_text = "ëŒ€í™” ê¸°ë¡ ì—†ìŒ"

        format_instructions = self.parser.get_format_instructions()

        prompt = self.prompt.format(
            history_text=history_text,
            current_query=current_query,
            format_instructions=format_instructions
        )

        llm_response = self.llm.invoke(prompt).content

        try:
            data = json.loads(llm_response)
            return data.get("queries", [current_query])
        except:
            print("âš  JSON íŒŒì‹± ì‹¤íŒ¨, ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©")
            return [current_query]


# ===============================================================
# 2) Multi Query Retriever
# ===============================================================

class MultiQueryRetriever:
    def __init__(self, vectorstore, query_rewriter):
        self.vectorstore = vectorstore
        self.history = []
        self.query_rewriter = query_rewriter

    def build_history_text(self):
        out = ""
        for h in self.history:
            out += f"[USER]\n{h['user_query']}\n"
            out += f"[REWRITTEN]\n{h['rewritten_queries']}\n"
            out += "[DOCS]\n"
            for d in h["retrieved_docs"]:
                out += f"- {d['content']}\n"
            out += f"[ANSWER]\n{h['final_answer']}\n"
            out += "-" * 30 + "\n"
        return out

    def retrieve(self, query: str, category: str, num_docs=5):

        history_text = self.build_history_text()
        rewritten_queries = self.query_rewriter.rewrite_query(history_text, query)

        print("ì¬ì‘ì„±ëœ ì¿¼ë¦¬:", rewritten_queries)

        all_docs = []
        seen = set()

        for q in rewritten_queries:
            # 1) ì¿¼ë¦¬ë‹¹ 5ê°œ ê²€ìƒ‰
            docs = self.vectorstore.similarity_search(
                q,
                k=num_docs,
                filter={"category": category}
            )

            if not docs:
                continue

            # 2) ì¿¼ë¦¬ë³„ CrossEncoder Top-2 ì¶”ì¶œ
            pairs = [(q, d.page_content) for d in docs]
            scores = self.cross_encoder.predict(pairs)

            # numpy/torch ë³€í™˜ ì²˜ë¦¬
            if hasattr(scores, "detach"):
                scores = scores.detach().cpu().numpy()
            scores = scores.squeeze().tolist()

            reranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)
    
            # â˜… ì—¬ê¸°ì„œ ì¿¼ë¦¬ë‹¹ Top-2ë§Œ ìœ ì§€
            top_docs = [doc for doc, score in reranked[:2]]

            # 3) ìµœì¢… í›„ë³´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ (ì¤‘ë³µ ì œê±°)
            for d in top_docs:
                if d.page_content not in seen:
                    all_docs.append(d)
                    seen.add(d.page_content)

        return all_docs, rewritten_queries

# ===============================================================
# 3) Fine-tuned Qwen2.5 HTP ëª¨ë¸ ê¸°ë°˜ RAG
# ===============================================================

from transformers import AutoModelForCausalLM, AutoTokenizer

class AdvancedConversationalRAG:
    def __init__(self, vectorstore, model_name="helena29/Qwen2.5_LoRA_for_HTP"):
        self.history = []
        self.query_rewriter = AdvancedQueryRewriter()
        self.retriever = MultiQueryRetriever(vectorstore, self.query_rewriter)
        self.retriever.cross_encoder = cross_encoder

        print("ğŸ”¥ Loading Qwen HTP Model:", model_name)
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.llm = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map="auto",
            torch_dtype="auto"
        )
        self.device = self.llm.device

        self.response_template = """
        You are a professional psychologist specializing in HTP interpretation.

        User Question:
        {query}

        Relevant Information:
        {context}

        Guidelines:
        1. If the user's question contains multiple queries, address each one clearly and separately.
        2. Base your answer only on the provided information. If information is insufficient, honestly state that you don't know.
        3. Provide your answer in Korean language.
        4. If there are original sources in the provided information, cite them appropriately.
        5. Explain possible psychological meanings in a professional manner.

        Answer:
        """

    def generate_response(self, prompt: str) -> str:

        messages = [
            {"role": "system", "content": "HTP ê²€ì‚¬ ì „ë¬¸ ì‹¬ë¦¬í•™ì ì—­í• "},
            {"role": "user", "content": prompt}
        ]

        formatted = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        inputs = self.tokenizer(formatted, return_tensors="pt")
        inputs = {k: v.to(self.device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = self.llm.generate(
                **inputs,
                max_new_tokens=1024,
                temperature=0.3,
                do_sample=True
            )

        return self.tokenizer.decode(
            outputs[0][inputs["input_ids"].shape[1]:],
            skip_special_tokens=True
        ).strip()

    def query(self, current_query: str, category: str):
        docs, rewritten_queries = self.retriever.retrieve(
            current_query,
            category=category
        )

        if docs:
            context = "\n\n".join(
                [f"ë¬¸ì„œ {i+1}:\n{doc.page_content}" for i, doc in enumerate(docs)]
            )
        else:
            context = "ê²€ìƒ‰ëœ ë¬¸ì„œ ì—†ìŒ"

        prompt = self.response_template.format(
            query=current_query,
            context=context
        )

        response = self.generate_response(prompt)

        record = {
            "user_query": current_query,
            "rewritten_queries": rewritten_queries,
            "retrieved_docs": [{"content": d.page_content} for d in docs],
            "final_answer": response,
        }

        self.history.append(record)
        self.retriever.history.append(record)

        return {
            "result": response,
            "rewritten_queries": rewritten_queries,
            "source_documents": docs
        }

