{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7bc62fa4-e00a-4150-b724-17729a9583cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Positive pairs: 573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 573/573 [00:05<00:00, 101.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triples: 1146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 287/287 [00:26<00:00, 10.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Loss: 0.1780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 287/287 [00:26<00:00, 10.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Loss: 0.0525\n",
      "=== Training Finished ===\n",
      "ëª¨ë¸ ì €ì¥ ì™„ë£Œ.\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 0. ë¼ì´ë¸ŒëŸ¬ë¦¬ & ê¸°ë³¸ ì„¤ì •\n",
    "# =========================================\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sentence_transformers import CrossEncoder, InputExample\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 1. í•™ìŠµ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "# =========================================\n",
    "dataset = load_dataset(\"HJUNN/Art_Therapy_caption_train_dataset\")[\"train\"]\n",
    "train_df = dataset.to_pandas()\n",
    "\n",
    "positive_pairs = list(zip(train_df[\"query\"], train_df[\"doc\"]))\n",
    "print(\"Positive pairs:\", len(positive_pairs))\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 2. í•˜ë“œ ë„¤ê±°í‹°ë¸Œ ìƒì„±\n",
    "# =========================================\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "embed = SentenceTransformer(\"HJUNN/bge-m3b-Art-Therapy-embedding-fine-tuning\")\n",
    "\n",
    "all_docs = list(dict.fromkeys(train_df[\"doc\"].tolist()))\n",
    "doc_emb = embed.encode(all_docs, convert_to_tensor=True, normalize_embeddings=True)\n",
    "\n",
    "from collections import defaultdict\n",
    "negatives = defaultdict(list)\n",
    "\n",
    "for q, pos_doc in tqdm(positive_pairs):\n",
    "    q_emb = embed.encode(q, convert_to_tensor=True, normalize_embeddings=True)\n",
    "    scores = util.cos_sim(q_emb, doc_emb)[0]\n",
    "    top_k = torch.topk(scores, 10)\n",
    "\n",
    "    for idx in top_k.indices:\n",
    "        cand = all_docs[idx]\n",
    "        if cand != pos_doc:\n",
    "            negatives[q].append(cand)\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 3. Triples ìƒì„± (query, pos, neg)\n",
    "# =========================================\n",
    "triples = []\n",
    "for q, pos in positive_pairs:\n",
    "    negs = negatives[q]\n",
    "    if len(negs) == 0:\n",
    "        continue\n",
    "    selected = random.sample(negs, min(2, len(negs)))\n",
    "\n",
    "    for neg in selected:\n",
    "        triples.append((q, pos, neg))\n",
    "\n",
    "print(\"Triples:\", len(triples))\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 4. Dataset & DataLoader\n",
    "# =========================================\n",
    "class TripleDataset(Dataset):\n",
    "    def __init__(self, triples):\n",
    "        self.triples = triples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        q, pos, neg = self.triples[idx]\n",
    "        return q, pos, neg\n",
    "\n",
    "\n",
    "dataset = TripleDataset(triples)\n",
    "loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 5. CrossEncoder ë¡œë“œ\n",
    "# =========================================\n",
    "model_name = \"BAAI/bge-reranker-base\"\n",
    "cross_encoder = CrossEncoder(model_name, num_labels=1, device=device)\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "optimizer = torch.optim.AdamW(cross_encoder.model.parameters(), lr=2e-5)\n",
    "\n",
    "margin = 1   # MarginRankingLoss ê°’\n",
    "loss_fn = torch.nn.MarginRankingLoss(margin=margin)\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 6. í•™ìŠµ ë£¨í”„ (ì§ì ‘ êµ¬í˜„)\n",
    "# =========================================\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    cross_encoder.model.train()\n",
    "\n",
    "    for q, pos, neg in tqdm(loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "\n",
    "        # ========= Positive input =========\n",
    "        pos_inputs = tokenizer(\n",
    "            list(q),\n",
    "            list(pos),\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=256,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        pos_output = cross_encoder.model(**pos_inputs)\n",
    "        pos_scores = pos_output.logits.squeeze()   # (batch,)\n",
    "\n",
    "        # ========= Negative input =========\n",
    "        neg_inputs = tokenizer(\n",
    "            list(q),\n",
    "            list(neg),\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=256,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        neg_output = cross_encoder.model(**neg_inputs)\n",
    "        neg_scores = neg_output.logits.squeeze()   # (batch,)\n",
    "\n",
    "        # ========= Margin Ranking Loss =========\n",
    "        # pos > neg ë˜ì–´ì•¼ í•˜ë¯€ë¡œ target = 1\n",
    "        target = torch.ones(pos_scores.size(0)).to(device)\n",
    "\n",
    "        loss = loss_fn(pos_scores, neg_scores, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}] Loss: {total_loss / len(loader):.4f}\")\n",
    "\n",
    "print(\"=== Training Finished ===\")\n",
    "\n",
    "# =========================================\n",
    "# 7. ëª¨ë¸ ì €ì¥\n",
    "# =========================================\n",
    "cross_encoder.save(\"./htp_cross_encoder_marginranking\")\n",
    "print(\"ëª¨ë¸ ì €ì¥ ì™„ë£Œ.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ee37c14-1952-4d2a-9d6e-cad4fdd85059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 8. í‰ê°€ í•¨ìˆ˜\n",
    "# =========================================\n",
    "import numpy as np\n",
    "\n",
    "def calculate_similarity(model, tokenizer, query, candidates, max_length=256):\n",
    "    model.eval()\n",
    "    raw_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for cand in candidates:\n",
    "            inputs = tokenizer(\n",
    "                query,\n",
    "                cand,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            logit = outputs.logits.squeeze().item()   # raw score\n",
    "            raw_scores.append(logit)\n",
    "\n",
    "    # ğŸ”¥ Softmax ì •ê·œí™”\n",
    "    scores_tensor = torch.tensor(raw_scores)\n",
    "    softmax_scores = torch.softmax(scores_tensor, dim=0).tolist()\n",
    "\n",
    "    # ğŸ”¥ candidate + softmax score ë¬¶ê¸°\n",
    "    results = []\n",
    "    for cand, prob in zip(candidates, softmax_scores):\n",
    "        results.append((cand, prob))\n",
    "\n",
    "    # í™•ë¥  ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬\n",
    "    results = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f910be",
   "metadata": {},
   "source": [
    "ê¸°ì¡´ pairwise ë°©ì‹ì€ pos_scoreë¥¼ ê³„ì† ì˜¬ë¦¬ê³  neg_scoreë¥¼ ê³„ì† ë‚´ë¦°ë‹¤\n",
    "ê·¸ë˜ì„œ logitsê°€ ë„ˆë¬´ ë°œì‚°í•´ì„œ sigmoid â†’ 0.999 vs 0.000 ìœ¼ë¡œ ê·¹ë‹¨ê°’ ë°œìƒ\n",
    "MarginRankingLossëŠ” pos_score â‰¥ neg_score + margin ê¹Œì§€ë§Œ í•™ìŠµí•˜ê³  ê·¸ ì´ìƒì€ fine-tuningì„ ë©ˆì¶¤\n",
    "ì¦‰, ê³¼ë„í•˜ê²Œ scoreë¥¼ ë²Œë¦¬ì§€ ì•ŠìŒ â†’ ë” ìì—°ìŠ¤ëŸ¬ìš´ ì ìˆ˜ ë¶„í¬ ìœ ì§€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24b95833-0b53-499c-896d-3290dc0f11e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== í•™ìŠµ í›„ Reranker ì ìˆ˜ ===\n",
      "\n",
      "[Query] ê·¸ë¦¼ì˜ ì§‘ì€ ë„“ê³  í¬ê²Œ ê·¸ë ¤ì ¸ ìˆìœ¼ë©°, ì¤‘ì•™ì— ìë¦¬ ì¡ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "1ìœ„ | ì ìˆ˜ 0.9331 | ë¬¸ì„œ: 1.<ì§‘ ê·¸ë¦¼ì˜ ë¶„ì„ê²°ê³¼ ì •ë¦¬>\n",
      "\n",
      "\n",
      " ì§‘ í¬ê¸° \n",
      "ìƒíƒœ: í¬ê²Œ ê·¸ë¦¬ëŠ” ê²½ìš° \n",
      "ê°€ëŠ¥í•œ ë¶„ì„: í”¼í—˜ìê°€ ìƒìƒë ¥ì´ ...\n",
      "2ìœ„ | ì ìˆ˜ 0.0184 | ë¬¸ì„œ: 3. í•­ëª©: ì§‘ì˜ í˜•íƒœ \n",
      "ìƒíƒœ: ì–‘ì˜¥ì§‘ \n",
      "ê°€ëŠ¥í•œ ë¶„ì„: ì–‘ì˜¥ì§‘ì„ ê·¸ë¦¬ëŠ” í”¼í—˜ìì˜ ê²½ìš° íƒ€ì¸ì˜ ë§ì— ì˜ ê³µê°í•´...\n",
      "3ìœ„ | ì ìˆ˜ 0.0155 | ë¬¸ì„œ: 7. í•­ëª©: ì°½ë¬¸ í¬ê¸° \n",
      "ìƒíƒœ: í¬ê²Œ \n",
      "ê°€ëŠ¥í•œ ë¶„ì„: í”¼í—˜ìê°€ í˜¸ê¸°ì‹¬ì´ ë§ì„ìˆ˜ë¡, ì•¼ì‹¬ì´ ìˆê³  ëª©í‘œì§€í–¥ì ì¼ìˆ˜...\n",
      "4ìœ„ | ì ìˆ˜ 0.0110 | ë¬¸ì„œ: 7. ê°€ì§€ëª¨ì–‘ \n",
      " - (ê°€ì§€ëª¨ì–‘ ì´ë¯¸ì§€) í”¼í—˜ìê°€ ì°½ì˜ì ì¼ìˆ˜ë¡, ê³µê°ì„ ì˜í• ìˆ˜ë¡ ìœ„ë¡œ ë¾°ì¡±í•œ ê°€ì§€ë¥¼ ê·¸ë¦¬ëŠ” ...\n",
      "5ìœ„ | ì ìˆ˜ 0.0070 | ë¬¸ì„œ: 5. ë™ë¬¼ìœ ë¬´ \n",
      "- ìˆë‹¤: í”¼í—˜ìê°€ ìì‹ ê°ì´ í´ìˆ˜ë¡ ë™ë¬¼ì„ ê·¸ë¦¬ëŠ” ê²½í–¥ì´ ìˆë‹¤.\n",
      " \n",
      "ë¿Œë¦¬ìœ„ì¹˜ \n",
      "- ì§€ë©´ì•„ë˜:...\n",
      "6ìœ„ | ì ìˆ˜ 0.0064 | ë¬¸ì„œ: 8. ì¤„ê¸°ì—ì„œ ë»—ì€ ìƒˆë¼ê°€ì§€ \n",
      "ìˆë‹¤: í”¼í—˜ìê°€ ì•¼ì‹¬ ìˆê³  ëª©í‘œì§€í–¥ì ì¼ìˆ˜ë¡ ì¤„ê¸°ì—ì„œ ë»—ì€ ìƒˆë¼ ê°€ì§€ë¥¼ ê·¸ë¦¬ëŠ” ...\n",
      "7ìœ„ | ì ìˆ˜ 0.0037 | ë¬¸ì„œ: 11. ë‹¤ë¦¬ê·¸ë¦¼\n",
      "- ì§§ë‹¤: í”¼í—˜ìê°€ í˜„ì‹¤ì ì¼ìˆ˜ë¡ ë‹¤ë¦¬ë¥¼ ì§§ê²Œ ê·¸ë¦¬ëŠ” ê²½í–¥ì´ ìˆë‹¤.\n",
      "\n",
      "\n",
      "ë‹¤ë¦¬ëª¨ì–‘\n",
      "- ë‚˜ë€íˆ ë¶™...\n",
      "8ìœ„ | ì ìˆ˜ 0.0031 | ë¬¸ì„œ: 9. ë‘ìƒë¹„ìœ¨\n",
      "- ì ë‹¹: í”¼í—˜ìê°€ ì„±ì·¨ë™ê¸°ê°€ ë†’ì„ìˆ˜ë¡ ë‘ìƒë¹„ìœ¨ì„ ê· í˜• ìˆê²Œ ê·¸ë¦¬ëŠ” ê²½í–¥ì´ ìˆë‹¤.\n",
      "- í¬ë‹¤: ...\n",
      "9ìœ„ | ì ìˆ˜ 0.0017 | ë¬¸ì„œ: 7. ëª©ê·¸ë¦¼\n",
      "- ìƒëµ: í”¼í—˜ìê°€ ìš°ìš¸í• ìˆ˜ë¡, ë°˜ì‚¬íšŒì  ì„±í–¥ì´ ê°•í• ìˆ˜ë¡, ë˜ëŠ” ì¸ë‚´ì‹¬ì´ ê°•í• ìˆ˜ë¡ ëª©ì„ ê·¸ë¦¬ì§€ ...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "[Query] ë‚˜ë¬´ ë¿Œë¦¬ëŠ” ë•… ì† ê¹Šì´ ìë¦¬ ì¡ê³  ìˆìœ¼ë©°, ì—¬ë°±ì—ëŠ” ì”ë””ê°€ ê³ ë¥´ê²Œ ê¹”ë ¤ ìˆìŠµë‹ˆë‹¤. ë™ë¬¼ì´ ë‚˜ë¬´ ì˜†ì— ì„œ ìˆëŠ” ëª¨ìŠµë„ ë³´ì…ë‹ˆë‹¤.\n",
      "1ìœ„ | ì ìˆ˜ 0.9981 | ë¬¸ì„œ: 5. ë™ë¬¼ìœ ë¬´ \n",
      "- ìˆë‹¤: í”¼í—˜ìê°€ ìì‹ ê°ì´ í´ìˆ˜ë¡ ë™ë¬¼ì„ ê·¸ë¦¬ëŠ” ê²½í–¥ì´ ìˆë‹¤.\n",
      " \n",
      "ë¿Œë¦¬ìœ„ì¹˜ \n",
      "- ì§€ë©´ì•„ë˜:...\n",
      "2ìœ„ | ì ìˆ˜ 0.0008 | ë¬¸ì„œ: 7. ëª©ê·¸ë¦¼\n",
      "- ìƒëµ: í”¼í—˜ìê°€ ìš°ìš¸í• ìˆ˜ë¡, ë°˜ì‚¬íšŒì  ì„±í–¥ì´ ê°•í• ìˆ˜ë¡, ë˜ëŠ” ì¸ë‚´ì‹¬ì´ ê°•í• ìˆ˜ë¡ ëª©ì„ ê·¸ë¦¬ì§€ ...\n",
      "3ìœ„ | ì ìˆ˜ 0.0005 | ë¬¸ì„œ: 8. ì¤„ê¸°ì—ì„œ ë»—ì€ ìƒˆë¼ê°€ì§€ \n",
      "ìˆë‹¤: í”¼í—˜ìê°€ ì•¼ì‹¬ ìˆê³  ëª©í‘œì§€í–¥ì ì¼ìˆ˜ë¡ ì¤„ê¸°ì—ì„œ ë»—ì€ ìƒˆë¼ ê°€ì§€ë¥¼ ê·¸ë¦¬ëŠ” ...\n",
      "4ìœ„ | ì ìˆ˜ 0.0002 | ë¬¸ì„œ: 7. ê°€ì§€ëª¨ì–‘ \n",
      " - (ê°€ì§€ëª¨ì–‘ ì´ë¯¸ì§€) í”¼í—˜ìê°€ ì°½ì˜ì ì¼ìˆ˜ë¡, ê³µê°ì„ ì˜í• ìˆ˜ë¡ ìœ„ë¡œ ë¾°ì¡±í•œ ê°€ì§€ë¥¼ ê·¸ë¦¬ëŠ” ...\n",
      "5ìœ„ | ì ìˆ˜ 0.0001 | ë¬¸ì„œ: 11. ë‹¤ë¦¬ê·¸ë¦¼\n",
      "- ì§§ë‹¤: í”¼í—˜ìê°€ í˜„ì‹¤ì ì¼ìˆ˜ë¡ ë‹¤ë¦¬ë¥¼ ì§§ê²Œ ê·¸ë¦¬ëŠ” ê²½í–¥ì´ ìˆë‹¤.\n",
      "\n",
      "\n",
      "ë‹¤ë¦¬ëª¨ì–‘\n",
      "- ë‚˜ë€íˆ ë¶™...\n",
      "6ìœ„ | ì ìˆ˜ 0.0001 | ë¬¸ì„œ: 3. í•­ëª©: ì§‘ì˜ í˜•íƒœ \n",
      "ìƒíƒœ: ì–‘ì˜¥ì§‘ \n",
      "ê°€ëŠ¥í•œ ë¶„ì„: ì–‘ì˜¥ì§‘ì„ ê·¸ë¦¬ëŠ” í”¼í—˜ìì˜ ê²½ìš° íƒ€ì¸ì˜ ë§ì— ì˜ ê³µê°í•´...\n",
      "7ìœ„ | ì ìˆ˜ 0.0001 | ë¬¸ì„œ: 9. ë‘ìƒë¹„ìœ¨\n",
      "- ì ë‹¹: í”¼í—˜ìê°€ ì„±ì·¨ë™ê¸°ê°€ ë†’ì„ìˆ˜ë¡ ë‘ìƒë¹„ìœ¨ì„ ê· í˜• ìˆê²Œ ê·¸ë¦¬ëŠ” ê²½í–¥ì´ ìˆë‹¤.\n",
      "- í¬ë‹¤: ...\n",
      "8ìœ„ | ì ìˆ˜ 0.0001 | ë¬¸ì„œ: 1.<ì§‘ ê·¸ë¦¼ì˜ ë¶„ì„ê²°ê³¼ ì •ë¦¬>\n",
      "\n",
      "\n",
      " ì§‘ í¬ê¸° \n",
      "ìƒíƒœ: í¬ê²Œ ê·¸ë¦¬ëŠ” ê²½ìš° \n",
      "ê°€ëŠ¥í•œ ë¶„ì„: í”¼í—˜ìê°€ ìƒìƒë ¥ì´ ...\n",
      "9ìœ„ | ì ìˆ˜ 0.0001 | ë¬¸ì„œ: 7. í•­ëª©: ì°½ë¬¸ í¬ê¸° \n",
      "ìƒíƒœ: í¬ê²Œ \n",
      "ê°€ëŠ¥í•œ ë¶„ì„: í”¼í—˜ìê°€ í˜¸ê¸°ì‹¬ì´ ë§ì„ìˆ˜ë¡, ì•¼ì‹¬ì´ ìˆê³  ëª©í‘œì§€í–¥ì ì¼ìˆ˜...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "[Query] ëª© ë¶€ë¶„ì´ ë‘ê»ê²Œ ê·¸ë ¤ì ¸ ìˆëŠ” ê²ƒì„ ê´€ì°°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "1ìœ„ | ì ìˆ˜ 0.9781 | ë¬¸ì„œ: 7. ëª©ê·¸ë¦¼\n",
      "- ìƒëµ: í”¼í—˜ìê°€ ìš°ìš¸í• ìˆ˜ë¡, ë°˜ì‚¬íšŒì  ì„±í–¥ì´ ê°•í• ìˆ˜ë¡, ë˜ëŠ” ì¸ë‚´ì‹¬ì´ ê°•í• ìˆ˜ë¡ ëª©ì„ ê·¸ë¦¬ì§€ ...\n",
      "2ìœ„ | ì ìˆ˜ 0.0136 | ë¬¸ì„œ: 8. ì¤„ê¸°ì—ì„œ ë»—ì€ ìƒˆë¼ê°€ì§€ \n",
      "ìˆë‹¤: í”¼í—˜ìê°€ ì•¼ì‹¬ ìˆê³  ëª©í‘œì§€í–¥ì ì¼ìˆ˜ë¡ ì¤„ê¸°ì—ì„œ ë»—ì€ ìƒˆë¼ ê°€ì§€ë¥¼ ê·¸ë¦¬ëŠ” ...\n",
      "3ìœ„ | ì ìˆ˜ 0.0041 | ë¬¸ì„œ: 5. ë™ë¬¼ìœ ë¬´ \n",
      "- ìˆë‹¤: í”¼í—˜ìê°€ ìì‹ ê°ì´ í´ìˆ˜ë¡ ë™ë¬¼ì„ ê·¸ë¦¬ëŠ” ê²½í–¥ì´ ìˆë‹¤.\n",
      " \n",
      "ë¿Œë¦¬ìœ„ì¹˜ \n",
      "- ì§€ë©´ì•„ë˜:...\n",
      "4ìœ„ | ì ìˆ˜ 0.0025 | ë¬¸ì„œ: 7. ê°€ì§€ëª¨ì–‘ \n",
      " - (ê°€ì§€ëª¨ì–‘ ì´ë¯¸ì§€) í”¼í—˜ìê°€ ì°½ì˜ì ì¼ìˆ˜ë¡, ê³µê°ì„ ì˜í• ìˆ˜ë¡ ìœ„ë¡œ ë¾°ì¡±í•œ ê°€ì§€ë¥¼ ê·¸ë¦¬ëŠ” ...\n",
      "5ìœ„ | ì ìˆ˜ 0.0008 | ë¬¸ì„œ: 9. ë‘ìƒë¹„ìœ¨\n",
      "- ì ë‹¹: í”¼í—˜ìê°€ ì„±ì·¨ë™ê¸°ê°€ ë†’ì„ìˆ˜ë¡ ë‘ìƒë¹„ìœ¨ì„ ê· í˜• ìˆê²Œ ê·¸ë¦¬ëŠ” ê²½í–¥ì´ ìˆë‹¤.\n",
      "- í¬ë‹¤: ...\n",
      "6ìœ„ | ì ìˆ˜ 0.0004 | ë¬¸ì„œ: 1.<ì§‘ ê·¸ë¦¼ì˜ ë¶„ì„ê²°ê³¼ ì •ë¦¬>\n",
      "\n",
      "\n",
      " ì§‘ í¬ê¸° \n",
      "ìƒíƒœ: í¬ê²Œ ê·¸ë¦¬ëŠ” ê²½ìš° \n",
      "ê°€ëŠ¥í•œ ë¶„ì„: í”¼í—˜ìê°€ ìƒìƒë ¥ì´ ...\n",
      "7ìœ„ | ì ìˆ˜ 0.0002 | ë¬¸ì„œ: 7. í•­ëª©: ì°½ë¬¸ í¬ê¸° \n",
      "ìƒíƒœ: í¬ê²Œ \n",
      "ê°€ëŠ¥í•œ ë¶„ì„: í”¼í—˜ìê°€ í˜¸ê¸°ì‹¬ì´ ë§ì„ìˆ˜ë¡, ì•¼ì‹¬ì´ ìˆê³  ëª©í‘œì§€í–¥ì ì¼ìˆ˜...\n",
      "8ìœ„ | ì ìˆ˜ 0.0002 | ë¬¸ì„œ: 11. ë‹¤ë¦¬ê·¸ë¦¼\n",
      "- ì§§ë‹¤: í”¼í—˜ìê°€ í˜„ì‹¤ì ì¼ìˆ˜ë¡ ë‹¤ë¦¬ë¥¼ ì§§ê²Œ ê·¸ë¦¬ëŠ” ê²½í–¥ì´ ìˆë‹¤.\n",
      "\n",
      "\n",
      "ë‹¤ë¦¬ëª¨ì–‘\n",
      "- ë‚˜ë€íˆ ë¶™...\n",
      "9ìœ„ | ì ìˆ˜ 0.0001 | ë¬¸ì„œ: 3. í•­ëª©: ì§‘ì˜ í˜•íƒœ \n",
      "ìƒíƒœ: ì–‘ì˜¥ì§‘ \n",
      "ê°€ëŠ¥í•œ ë¶„ì„: ì–‘ì˜¥ì§‘ì„ ê·¸ë¦¬ëŠ” í”¼í—˜ìì˜ ê²½ìš° íƒ€ì¸ì˜ ë§ì— ì˜ ê³µê°í•´...\n",
      "----------------------------------------------------------------------\n",
      "ì €ì¥ ì™„ë£Œ: ./htp_bge_cross_encoder\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 9. ê²€ì¦ ë°ì´í„° í‰ê°€\n",
    "# =========================================\n",
    "valid_dataset = load_dataset(\"HJUNN/Art_Therapy_caption_valid_dataset\")[\"validation\"]\n",
    "valid_df = valid_dataset.to_pandas()\n",
    "\n",
    "test_queries = [\n",
    "    valid_df[\"query\"][0],\n",
    "    valid_df[\"query\"][55],\n",
    "    valid_df[\"query\"][85]\n",
    "]\n",
    "\n",
    "test_candidates = [\n",
    "    valid_df[\"doc\"][0], valid_df[\"doc\"][7], valid_df[\"doc\"][20],\n",
    "    valid_df[\"doc\"][55], valid_df[\"doc\"][60], valid_df[\"doc\"][65],\n",
    "    valid_df[\"doc\"][85], valid_df[\"doc\"][90], valid_df[\"doc\"][95],\n",
    "]\n",
    "\n",
    "print(\"\\n=== í•™ìŠµ í›„ Reranker ì ìˆ˜ ===\")\n",
    "for q in test_queries:\n",
    "    print(f\"\\n[Query] {q}\")\n",
    "    scores = calculate_similarity(model, tokenizer, q, test_candidates)\n",
    "    for i, (doc, s) in enumerate(scores, start=1):\n",
    "        print(f\"{i}ìœ„ | ì ìˆ˜ {s:.4f} | ë¬¸ì„œ: {doc[:60]}...\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "model.save_pretrained(\"./htp_bge_cross_encoder\")\n",
    "tokenizer.save_pretrained(\"./htp_bge_cross_encoder\")\n",
    "print(\"ì €ì¥ ì™„ë£Œ: ./htp_bge_cross_encoder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3358f44-df21-4234-8ea6-10033fac2ae9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0c4162-a515-46df-adb1-1bacc515282b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
