{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc98b0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\subprocess.py\", line 1515, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "  File \"c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\codecs.py\", line 322, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc0 in position 6: invalid start byte\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "import bitsandbytes as bnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a2b3119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU ì •ë³´:\n",
      "GPU Name: NVIDIA GeForce RTX 4060\n",
      "GPU Memory: 8.00 GB\n"
     ]
    }
   ],
   "source": [
    "# ========== GPU ë©”ëª¨ë¦¬ í™•ì¸ ==========\n",
    "print(\"GPU ì •ë³´:\")\n",
    "print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eddd9d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== ì„¤ì • ==========\n",
    "@dataclass\n",
    "class Config:\n",
    "    # ëª¨ë¸ ì„¤ì •\n",
    "    model_name: str = \"Qwen/Qwen2.5-1.5B-Instruct\"  \n",
    "    # model_name: str = \"meta-llama/Llama-2-7b-hf\"  # 7B ëª¨ë¸ (16GB ì´ìƒ)\n",
    "    \n",
    "    # ë°ì´í„° ê²½ë¡œ\n",
    "    train_data_path: str = \"HTP_data.jsonl\"\n",
    "    \n",
    "    # ì¶œë ¥ ê²½ë¡œ\n",
    "    output_dir: str = \"./htp_lora_model\"\n",
    "    \n",
    "    # ë°°ì¹˜ í¬ê¸° (RTX 4060 8GBìš©)\n",
    "    per_device_train_batch_size: int = 2\n",
    "    per_device_eval_batch_size: int = 4\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    \n",
    "    # í•™ìŠµ ì„¤ì •\n",
    "    num_train_epochs: int = 10\n",
    "    learning_rate: float = 2e-4\n",
    "    warmup_ratio: float = 0.03\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ ê¸¸ì´\n",
    "    max_seq_length: int = 512\n",
    "    \n",
    "    # LoRA ì„¤ì •\n",
    "    lora_r: int = 8\n",
    "    lora_alpha: int = 32\n",
    "    lora_dropout: float = 0.05\n",
    "    \n",
    "    # 8-bit ì–‘ìí™” (ë©”ëª¨ë¦¬ ì ˆê°)\n",
    "    use_8bit: bool = True\n",
    "    \n",
    "    # ê¸°íƒ€\n",
    "    seed: int = 42\n",
    "    fp16: bool = True  # Mixed precision training\n",
    "\n",
    "\n",
    "# ========== ë°ì´í„° ë¡œë”© ==========\n",
    "def load_jsonl_data(file_path: str) -> list:\n",
    "    \"\"\"JSONL íŒŒì¼ ì½ê¸°\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "\n",
    "def prepare_dataset(data: list, tokenizer, config: Config) -> Dataset:\n",
    "    \"\"\"ë°ì´í„°ì…‹ ì¤€ë¹„\"\"\"\n",
    "    \n",
    "    def formatting_func(examples):\n",
    "        # inputê³¼ outputì„ í•©ì³ì„œ í•™ìŠµ ë°ì´í„° ìƒì„±\n",
    "        texts = []\n",
    "        for inp, out in zip(examples['input'], examples['output']):\n",
    "            text = f\"HTP í•´ì„ ì…ë ¥: {inp}\\n\\nHTP í•´ì„ ì¶œë ¥: {out}\"\n",
    "            texts.append(text)\n",
    "        \n",
    "        # í† í°í™”\n",
    "        tokenized = tokenizer(\n",
    "            texts,\n",
    "            max_length=config.max_seq_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        \n",
    "        # labels ì„¤ì • (input_idsì™€ ë™ì¼í•˜ê²Œ)\n",
    "        tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "        \n",
    "        return tokenized\n",
    "    \n",
    "    # Datasetìœ¼ë¡œ ë³€í™˜\n",
    "    dataset = Dataset.from_dict({\n",
    "        'input': [item['input'] for item in data],\n",
    "        'output': [item['output'] for item in data],\n",
    "    })\n",
    "    \n",
    "    # ì²˜ë¦¬\n",
    "    processed_dataset = dataset.map(\n",
    "        formatting_func,\n",
    "        batched=True,\n",
    "        batch_size=100,\n",
    "        remove_columns=['input', 'output'],\n",
    "        desc=\"í¬ë§¤íŒ… ì¤‘...\",\n",
    "    )\n",
    "    \n",
    "    return processed_dataset\n",
    "\n",
    "\n",
    "# ========== ëª¨ë¸ ë¡œë”© ë° LoRA ì„¤ì • ==========\n",
    "def setup_model_and_tokenizer(config: Config):\n",
    "    \"\"\"ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì„¤ì •\"\"\"\n",
    "    \n",
    "    print(f\"ëª¨ë¸ ë¡œë”©: {config.model_name}\")\n",
    "    \n",
    "    # í† í¬ë‚˜ì´ì € ë¡œë”©\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # ëª¨ë¸ ë¡œë”© (8-bit ì–‘ìí™”)\n",
    "    if config.use_8bit:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            config.model_name,\n",
    "            load_in_8bit=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        \n",
    "        # 8-bit ì–‘ìí™” ëª¨ë¸ ì¤€ë¹„\n",
    "        model = prepare_model_for_kbit_training(\n",
    "            model,\n",
    "            use_gradient_checkpointing=True,\n",
    "        )\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            config.model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "    \n",
    "    # LoRA ì„¤ì •\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=config.lora_r,\n",
    "        lora_alpha=config.lora_alpha,\n",
    "        lora_dropout=config.lora_dropout,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],  # ëª¨ë¸ì— ë”°ë¼ ìˆ˜ì • í•„ìš”\n",
    "    )\n",
    "    \n",
    "    # LoRA ì ìš©\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° í™•ì¸\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19d96d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° ë¡œë”© ì¤‘...\n",
      "ë¡œë“œëœ ìƒ˜í”Œ ìˆ˜: 1453\n",
      "ëª¨ë¸ ë¡œë”©: Qwen/Qwen2.5-1.5B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,089,536 || all params: 1,544,803,840 || trainable%: 0.0705\n",
      "ë°ì´í„°ì…‹ ì¤€ë¹„ ì¤‘...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac9eec23730445c682fca634ff38d133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "í¬ë§¤íŒ… ì¤‘...:   0%|          | 0/1453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•™ìŠµ ìƒ˜í”Œ: 1307, ê²€ì¦ ìƒ˜í”Œ: 146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•™ìŠµ ì‹œì‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1640' max='1640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1640/1640 2:27:15, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.210900</td>\n",
       "      <td>0.213300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.188800</td>\n",
       "      <td>0.204048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.184000</td>\n",
       "      <td>0.199951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.191600</td>\n",
       "      <td>0.197915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.171100</td>\n",
       "      <td>0.195824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.168600</td>\n",
       "      <td>0.196182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.162700</td>\n",
       "      <td>0.195154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.156100</td>\n",
       "      <td>0.196050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.150600</td>\n",
       "      <td>0.196372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.196334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëª¨ë¸ ì €ì¥ ì¤‘...\n",
      "âœ“ íŒŒì¸íŠœë‹ ì™„ë£Œ: ./htp_lora_model\n"
     ]
    }
   ],
   "source": [
    "# ========== ë©”ì¸ í•™ìŠµ í•¨ìˆ˜ ==========\n",
    "config = Config()\n",
    "\n",
    "# ì‹œë“œ ì„¤ì •\n",
    "transformers.set_seed(config.seed)\n",
    "\n",
    "# ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "Path(config.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1. ë°ì´í„° ë¡œë“œ\n",
    "print(\"ë°ì´í„° ë¡œë”© ì¤‘...\")\n",
    "raw_data = load_jsonl_data(config.train_data_path)\n",
    "print(f\"ë¡œë“œëœ ìƒ˜í”Œ ìˆ˜: {len(raw_data)}\")\n",
    "\n",
    "# 2. ëª¨ë¸, í† í¬ë‚˜ì´ì € ì„¤ì •\n",
    "model, tokenizer = setup_model_and_tokenizer(config)\n",
    "\n",
    "# 3. ë°ì´í„°ì…‹ ì¤€ë¹„\n",
    "print(\"ë°ì´í„°ì…‹ ì¤€ë¹„ ì¤‘...\")\n",
    "dataset = prepare_dataset(raw_data, tokenizer, config)\n",
    "\n",
    "# í•™ìŠµ/ê²€ì¦ ë¶„í•  (9:1)\n",
    "split_dataset = dataset.train_test_split(test_size=0.1, seed=config.seed)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "print(f\"í•™ìŠµ ìƒ˜í”Œ: {len(train_dataset)}, ê²€ì¦ ìƒ˜í”Œ: {len(eval_dataset)}\")\n",
    "\n",
    "# 4. í•™ìŠµ ì„¤ì •\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=config.output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    \n",
    "    # ë°°ì¹˜ í¬ê¸°\n",
    "    per_device_train_batch_size=config.per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=config.per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    \n",
    "    # ì—í­\n",
    "    num_train_epochs=config.num_train_epochs,\n",
    "    \n",
    "    # í•™ìŠµë¥ \n",
    "    learning_rate=config.learning_rate,\n",
    "    warmup_ratio=config.warmup_ratio,\n",
    "    \n",
    "    # ì˜µí‹°ë§ˆì´ì €\n",
    "    optim=\"paged_adamw_32bit\",  # 8-bit ìµœì í™”\n",
    "    \n",
    "    # ì €ì¥ ë° ë¡œê¹…\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    \n",
    "    # ê³„ì‚° ìµœì í™”\n",
    "    fp16=config.fp16,\n",
    "    gradient_checkpointing=True,\n",
    "    max_grad_norm=1.0,\n",
    "    \n",
    "    # ê¸°íƒ€\n",
    "    seed=config.seed,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "# 5. Trainer ì„¤ì •\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8),\n",
    ")\n",
    "\n",
    "# 6. í•™ìŠµ ì‹œì‘\n",
    "print(\"í•™ìŠµ ì‹œì‘...\")\n",
    "trainer.train()\n",
    "\n",
    "# 7. ëª¨ë¸ ì €ì¥\n",
    "print(\"ëª¨ë¸ ì €ì¥ ì¤‘...\")\n",
    "model.save_pretrained(config.output_dir)\n",
    "tokenizer.save_pretrained(config.output_dir)\n",
    "\n",
    "print(f\"âœ“ íŒŒì¸íŠœë‹ ì™„ë£Œ: {config.output_dir}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd119469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ HTP ëª¨ë¸ ë””ë²„ê¹… ì‹œì‘\n",
      "\n",
      "==================================================\n",
      "ë‹¨ê³„ 1: ì„¤ì • í™•ì¸\n",
      "==================================================\n",
      "ğŸ”§ CUDA ì‚¬ìš© ê°€ëŠ¥: True\n",
      "ğŸ”§ GPU ì´ë¦„: NVIDIA GeForce RTX 4060\n",
      "ğŸ”§ GPU ë©”ëª¨ë¦¬: 8.00 GB\n",
      "\n",
      "ë² ì´ìŠ¤ ëª¨ë¸: Qwen/Qwen2.5-1.5B-Instruct\n",
      "\n",
      "==================================================\n",
      "ë‹¨ê³„ 2: ëª¨ë¸ ë¡œë“œ (ì–‘ìí™” ì—†ì´)\n",
      "==================================================\n",
      "âœ“ FP16ìœ¼ë¡œ ë¡œë“œ (ë¹ ë¦„)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Exception in thread Thread-4 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\subprocess.py\", line 1515, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "  File \"c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\codecs.py\", line 322, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc0 in position 6: invalid start byte\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ LoRA ê°€ì¤‘ì¹˜ ë³‘í•© ì¤‘...\n",
      "\n",
      "==================================================\n",
      "ğŸ” ë””ë°”ì´ìŠ¤ ë°°ì¹˜ í™•ì¸\n",
      "==================================================\n",
      "  cuda:0: 338 íŒŒë¼ë¯¸í„°\n",
      "\n",
      "âœ“ ì£¼ ë””ë°”ì´ìŠ¤: cuda:0\n",
      "\n",
      "âœ“ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\n",
      "\n",
      "==================================================\n",
      "ğŸ”¤ í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸\n",
      "==================================================\n",
      "'ì•ˆë…•í•˜ì„¸ìš”' â†’ 3 í† í°\n",
      "'ë‚˜ë¬´ê°€ í¬ê³  ê°€ì§€ê°€ ë§ìœ¼ë©° ë¿Œë¦¬ê°€ ê¹Šê²Œ í‘œí˜„ë¨' â†’ 19 í† í°\n",
      "'Hello world' â†’ 2 í† í°\n",
      "\n",
      "Pad token: <|im_end|> (ID: 151645)\n",
      "EOS token: <|im_end|> (ID: 151645)\n",
      "\n",
      "======================================================================\n",
      "í…ŒìŠ¤íŠ¸ 1: ì˜ì–´ ìƒì„±\n",
      "======================================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ§ª ê°„ë‹¨ ìƒì„± í…ŒìŠ¤íŠ¸\n",
      "==================================================\n",
      "ì…ë ¥ í…ìŠ¤íŠ¸: The tree is\n",
      "ì…ë ¥ í† í° ìˆ˜: 3\n",
      "ì…ë ¥ ë””ë°”ì´ìŠ¤: cuda:0\n",
      "ëª¨ë¸ ë””ë°”ì´ìŠ¤: cuda:0\n",
      "\n",
      "â±ï¸ ìƒì„± ì‹œì‘... (max_new_tokens=20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ ìƒì„± ì™„ë£Œ! (1.89ì´ˆ)\n",
      "ìƒì„±ëœ í† í° ìˆ˜: 20\n",
      "ì´ˆë‹¹ í† í°: 10.58 tokens/sec\n",
      "\n",
      "ê²°ê³¼:\n",
      " very tall, and the branches are lush. The leaves are green and shiny, with a strong sense\n",
      "\n",
      "======================================================================\n",
      "í…ŒìŠ¤íŠ¸ 2: í•œêµ­ì–´ ìƒì„±\n",
      "======================================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ§ª ê°„ë‹¨ ìƒì„± í…ŒìŠ¤íŠ¸\n",
      "==================================================\n",
      "ì…ë ¥ í…ìŠ¤íŠ¸: ë‚˜ë¬´ê°€\n",
      "ì…ë ¥ í† í° ìˆ˜: 3\n",
      "ì…ë ¥ ë””ë°”ì´ìŠ¤: cuda:0\n",
      "ëª¨ë¸ ë””ë°”ì´ìŠ¤: cuda:0\n",
      "\n",
      "â±ï¸ ìƒì„± ì‹œì‘... (max_new_tokens=20)\n",
      "âœ“ ìƒì„± ì™„ë£Œ! (1.32ì´ˆ)\n",
      "ìƒì„±ëœ í† í° ìˆ˜: 20\n",
      "ì´ˆë‹¹ í† í°: 15.11 tokens/sec\n",
      "\n",
      "ê²°ê³¼:\n",
      "ë¬´í•œë¬´í•œì •ìˆ˜\n",
      "\n",
      "Given the function \\( f(x) = \\frac{1}{\n",
      "\n",
      "======================================================================\n",
      "í…ŒìŠ¤íŠ¸ 3: Qwen í”„ë¡¬í”„íŠ¸ í˜•ì‹\n",
      "======================================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ§ª ê°„ë‹¨ ìƒì„± í…ŒìŠ¤íŠ¸\n",
      "==================================================\n",
      "ì…ë ¥ í…ìŠ¤íŠ¸: <|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Say hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "ì…ë ¥ í† í° ìˆ˜: 21\n",
      "ì…ë ¥ ë””ë°”ì´ìŠ¤: cuda:0\n",
      "ëª¨ë¸ ë””ë°”ì´ìŠ¤: cuda:0\n",
      "\n",
      "â±ï¸ ìƒì„± ì‹œì‘... (max_new_tokens=20)\n",
      "âœ“ ìƒì„± ì™„ë£Œ! (0.71ì´ˆ)\n",
      "ìƒì„±ëœ í† í° ìˆ˜: 10\n",
      "ì´ˆë‹¹ í† í°: 14.13 tokens/sec\n",
      "\n",
      "ê²°ê³¼:\n",
      "Hello! How can I assist you today?\n",
      "\n",
      "======================================================================\n",
      "âœ“ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def check_device_placement(model):\n",
    "    \"\"\"ëª¨ë¸ì´ ì‹¤ì œë¡œ ì–´ëŠ ë””ë°”ì´ìŠ¤ì— ìˆëŠ”ì§€ í™•ì¸\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"ğŸ” ë””ë°”ì´ìŠ¤ ë°°ì¹˜ í™•ì¸\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    devices = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        device = str(param.device)\n",
    "        devices[device] = devices.get(device, 0) + 1\n",
    "    \n",
    "    for device, count in devices.items():\n",
    "        print(f\"  {device}: {count} íŒŒë¼ë¯¸í„°\")\n",
    "    \n",
    "    print(f\"\\nâœ“ ì£¼ ë””ë°”ì´ìŠ¤: {next(model.parameters()).device}\")\n",
    "    return next(model.parameters()).device\n",
    "\n",
    "def load_finetuned_model_debug(model_path, base_model_name=None, use_4bit=False):\n",
    "    \"\"\"\n",
    "    ë””ë²„ê¹…ìš© ëª¨ë¸ ë¡œë“œ (4-bit ì–‘ìí™” ê¸°ë³¸ OFF)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"=\" * 50)\n",
    "        print(\"ë‹¨ê³„ 1: ì„¤ì • í™•ì¸\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸\n",
    "        print(f\"ğŸ”§ CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"ğŸ”§ GPU ì´ë¦„: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"ğŸ”§ GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "        \n",
    "        # ì„¤ì • íŒŒì¼ ë¡œë“œ\n",
    "        adapter_config_path = os.path.join(model_path, \"adapter_config.json\")\n",
    "        if os.path.exists(adapter_config_path):\n",
    "            with open(adapter_config_path, 'r') as f:\n",
    "                config = json.load(f)\n",
    "                base_model_name = config.get(\"base_model_name_or_path\", base_model_name)\n",
    "        \n",
    "        if base_model_name is None:\n",
    "            base_model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "        \n",
    "        print(f\"\\në² ì´ìŠ¤ ëª¨ë¸: {base_model_name}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"ë‹¨ê³„ 2: ëª¨ë¸ ë¡œë“œ (ì–‘ìí™” ì—†ì´)\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # ì–‘ìí™” ì—†ì´ ë¡œë“œ (ë””ë²„ê¹…ìš©)\n",
    "        if use_4bit:\n",
    "            print(\"âš ï¸ 4-bit ì–‘ìí™” ì‚¬ìš© (ëŠë¦´ ìˆ˜ ìˆìŒ)\")\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\"\n",
    "            )\n",
    "        else:\n",
    "            print(\"âœ“ FP16ìœ¼ë¡œ ë¡œë“œ (ë¹ ë¦„)\")\n",
    "            quantization_config = None\n",
    "        \n",
    "        model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16,\n",
    "            quantization_config=quantization_config,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        print(\"\\nâœ“ LoRA ê°€ì¤‘ì¹˜ ë³‘í•© ì¤‘...\")\n",
    "        model = model.merge_and_unload()\n",
    "        \n",
    "        # ë””ë°”ì´ìŠ¤ í™•ì¸\n",
    "        device = check_device_placement(model)\n",
    "        \n",
    "        print(\"\\nâœ“ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
    "        return model, tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "def simple_generate(model, tokenizer, text, max_new_tokens=50):\n",
    "    \"\"\"\n",
    "    ìµœì†Œí•œì˜ ì„¤ì •ìœ¼ë¡œ ìƒì„± í…ŒìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"ğŸ§ª ê°„ë‹¨ ìƒì„± í…ŒìŠ¤íŠ¸\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # ì…ë ¥ ì¤€ë¹„\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = inputs['input_ids'].to(model.device)\n",
    "    \n",
    "    print(f\"ì…ë ¥ í…ìŠ¤íŠ¸: {text}\")\n",
    "    print(f\"ì…ë ¥ í† í° ìˆ˜: {input_ids.shape[1]}\")\n",
    "    print(f\"ì…ë ¥ ë””ë°”ì´ìŠ¤: {input_ids.device}\")\n",
    "    print(f\"ëª¨ë¸ ë””ë°”ì´ìŠ¤: {next(model.parameters()).device}\")\n",
    "    \n",
    "    # ìƒì„± ì‹œì‘\n",
    "    print(f\"\\nâ±ï¸ ìƒì„± ì‹œì‘... (max_new_tokens={max_new_tokens})\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            # ë§¤ìš° ê°„ë‹¨í•œ ìƒì„± ì„¤ì •\n",
    "            outputs = model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,  # greedy decoding (ê°€ì¥ ë¹ ë¦„)\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"âœ“ ìƒì„± ì™„ë£Œ! ({elapsed:.2f}ì´ˆ)\")\n",
    "        \n",
    "        # ê²°ê³¼ ë””ì½”ë”©\n",
    "        generated_ids = outputs[0][input_ids.shape[1]:]\n",
    "        result = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "        \n",
    "        print(f\"ìƒì„±ëœ í† í° ìˆ˜: {len(generated_ids)}\")\n",
    "        print(f\"ì´ˆë‹¹ í† í°: {len(generated_ids)/elapsed:.2f} tokens/sec\")\n",
    "        print(f\"\\nê²°ê³¼:\\n{result}\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"âŒ ìƒì„± ì‹¤íŒ¨ ({elapsed:.2f}ì´ˆ ê²½ê³¼)\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "def test_tokenizer(tokenizer):\n",
    "    \"\"\"í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"ğŸ”¤ í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    test_texts = [\n",
    "        \"ì•ˆë…•í•˜ì„¸ìš”\",\n",
    "        \"ë‚˜ë¬´ê°€ í¬ê³  ê°€ì§€ê°€ ë§ìœ¼ë©° ë¿Œë¦¬ê°€ ê¹Šê²Œ í‘œí˜„ë¨\",\n",
    "        \"Hello world\"\n",
    "    ]\n",
    "    \n",
    "    for text in test_texts:\n",
    "        tokens = tokenizer.encode(text)\n",
    "        print(f\"'{text}' â†’ {len(tokens)} í† í°\")\n",
    "    \n",
    "    print(f\"\\nPad token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "    print(f\"EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
    "\n",
    "# ë©”ì¸ ì‹¤í–‰\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        print(\"ğŸš€ HTP ëª¨ë¸ ë””ë²„ê¹… ì‹œì‘\\n\")\n",
    "        \n",
    "        # 1ë‹¨ê³„: ëª¨ë¸ ë¡œë“œ\n",
    "        model, tokenizer = load_finetuned_model_debug(\n",
    "            \"./htp_lora_model\",\n",
    "            base_model_name=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "            use_4bit=False  # ë””ë²„ê¹…ì‹œ Falseë¡œ!\n",
    "        )\n",
    "        \n",
    "        # 2ë‹¨ê³„: í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸\n",
    "        test_tokenizer(tokenizer)\n",
    "        \n",
    "        # 3ë‹¨ê³„: ì´ˆê°„ë‹¨ ìƒì„± í…ŒìŠ¤íŠ¸ (ì˜ì–´)\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"í…ŒìŠ¤íŠ¸ 1: ì˜ì–´ ìƒì„±\")\n",
    "        print(\"=\" * 70)\n",
    "        simple_generate(model, tokenizer, \"The tree is\", max_new_tokens=20)\n",
    "        \n",
    "        # 4ë‹¨ê³„: í•œêµ­ì–´ ìƒì„± í…ŒìŠ¤íŠ¸\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"í…ŒìŠ¤íŠ¸ 2: í•œêµ­ì–´ ìƒì„±\")\n",
    "        print(\"=\" * 70)\n",
    "        simple_generate(model, tokenizer, \"ë‚˜ë¬´ê°€\", max_new_tokens=20)\n",
    "        \n",
    "        # 5ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ í˜•ì‹ìœ¼ë¡œ í…ŒìŠ¤íŠ¸\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"í…ŒìŠ¤íŠ¸ 3: Qwen í”„ë¡¬í”„íŠ¸ í˜•ì‹\")\n",
    "        print(\"=\" * 70)\n",
    "        qwen_prompt = \"\"\"<|im_start|>system\n",
    "You are a helpful assistant.<|im_end|>\n",
    "<|im_start|>user\n",
    "Say hello<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "        simple_generate(model, tokenizer, qwen_prompt, max_new_tokens=20)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"âœ“ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ ì—ëŸ¬ ë°œìƒ: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce83a2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "ğŸš€ HTP ëª¨ë¸ ë¡œë“œ ì¤‘...\n",
      "==================================================\n",
      "âœ“ GPU: NVIDIA GeForce RTX 4060\n",
      "âœ“ GPU ë©”ëª¨ë¦¬: 8.0 GB\n",
      "\n",
      "ë² ì´ìŠ¤ ëª¨ë¸: Qwen/Qwen2.5-1.5B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\subprocess.py\", line 1515, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "  File \"c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\codecs.py\", line 322, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc0 in position 6: invalid start byte\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA ê°€ì¤‘ì¹˜ ë³‘í•© ì¤‘...\n",
      "âœ“ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\n",
      "\n",
      "\n",
      "======================================================================\n",
      "ë°°ì¹˜ í•´ì„ í…ŒìŠ¤íŠ¸\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "[1/10] í•´ì„ ì¤‘...\n",
      "======================================================================\n",
      "\n",
      "ì…ë ¥: The tree is large with many branches and deeply drawn roots\n",
      "í•´ì„: This indicates significant growth potential and potentially underlying issues related to dependency or difficulty establishing boundaries. The expansive nature of both the tree and its roots suggests an overwhelming need for external support and resources, possibly reflecting feelings of insecurity or lack of self-esteem. It could also signify difficulties in forming healthy relationships due to excessive reliance on others.\n",
      "\n",
      "======================================================================\n",
      "[2/10] í•´ì„ ì¤‘...\n",
      "======================================================================\n",
      "\n",
      "ì…ë ¥: The house is small with no doors or windows, and no smoke from the chimney\n",
      "í•´ì„: This suggests an underlying feeling of isolation, detachment, and possibly unresolved grief or loss. The lack of visual stimuli within the structure could indicate difficulty forming relationships or experiencing feelings of emptiness. It may also reflect difficulties coping with external pressures or challenges.\n",
      "\n",
      "======================================================================\n",
      "[3/10] í•´ì„ ì¤‘...\n",
      "======================================================================\n",
      "\n",
      "ì…ë ¥: The person's hands are hidden behind their back and the face has no expression\n",
      "í•´ì„: In this context, it suggests defensiveness or difficulty expressing emotions openly. The obscured hand position might indicate anxiety about social interaction or fear of vulnerability. Additionally, without facial expression, there could be a reluctance to engage with others due to perceived judgment or insecurity regarding oneâ€™s own feelings. This is often associated with depression and low self-esteem within psychology assessments.\n",
      "\n",
      "======================================================================\n",
      "[4/10] í•´ì„ ì¤‘...\n",
      "======================================================================\n",
      "\n",
      "ì…ë ¥: The tree trunk has holes and the branches are drooping downward\n",
      "í•´ì„: This indicates potential feelings of insecurity or instability in the individual's life experiences. The broken structure suggests difficulties coping with external pressures and potentially unresolved conflicts from past events. It could also represent underlying anxieties about future challenges. The overall impression points to an emotional state characterized by vulnerability and perhaps difficulty trusting oneself or others.\n",
      "\n",
      "======================================================================\n",
      "[5/10] í•´ì„ ì¤‘...\n",
      "======================================================================\n",
      "\n",
      "ì…ë ¥: The house has an excessively large roof with many complex decorations\n",
      "í•´ì„: This suggests potential issues related to control, power dynamics within relationships, or difficulties managing overwhelming responsibilities at home due to excessive decoration reflecting an inflated sense of self-importance. The complexity indicates cognitive overactivity and potentially difficulty concentrating on practical tasks necessary for maintaining family stability.\n",
      "\n",
      "======================================================================\n",
      "[6/10] í•´ì„ ì¤‘...\n",
      "======================================================================\n",
      "\n",
      "ì…ë ¥: The tree is drawn in the corner with very thin trunk and sparse leaves\n",
      "í•´ì„: This suggests a potential difficulty in establishing stable foundations or relationships. The lack of robustness indicates underlying instability and possibly feelings of insecurity related to emotional connections and support systems. It may reflect an avoidance strategy for vulnerability and dependence. Further exploration into anxiety-related themes could be warranted.\n",
      "\n",
      "======================================================================\n",
      "[7/10] í•´ì„ ì¤‘...\n",
      "======================================================================\n",
      "\n",
      "ì…ë ¥: The person is drawn very small at the bottom of the page with no facial features\n",
      "í•´ì„: This suggests significant regression to earlier developmental stages or difficulty processing complex information due to fear or anxiety, potentially indicative of dissociation or disconnection from reality. The lack of detail reflects an inability to engage fully with external stimuli.\n",
      "\n",
      "======================================================================\n",
      "[8/10] í•´ì„ ì¤‘...\n",
      "======================================================================\n",
      "\n",
      "ì…ë ¥: The house has heavily shaded walls and windows with bars\n",
      "í•´ì„: This indicates potential difficulties related to trust, vulnerability, or feelings of being restricted by relationships. The shading suggests avoidance of open communication and potentially underlying anxiety about personal connections. It could also represent an internalized fear of judgment or rejection. This feature is commonly associated with depressive symptoms and interpersonal distress within personality disorders.\n",
      "\n",
      "======================================================================\n",
      "[9/10] í•´ì„ ì¤‘...\n",
      "======================================================================\n",
      "\n",
      "ì…ë ¥: The tree has broken branches and appears to be leaning to one side\n",
      "í•´ì„: This indicates potential instability or difficulty managing stress within the individual's life. The asymmetrical structure of both the house and person could signify an imbalance in their sense of self-worth and emotional regulation skills. It may reflect underlying anxiety or feelings of being overwhelmed by external pressures. Further exploration into coping mechanisms and support systems would be warranted.\n",
      "\n",
      "======================================================================\n",
      "[10/10] í•´ì„ ì¤‘...\n",
      "======================================================================\n",
      "\n",
      "ì…ë ¥: The person has unusually large hands and feet with exaggerated details\n",
      "í•´ì„: This indicates potential difficulties regulating emotions or experiencing heightened sensory perceptions â€“ specifically, an overabundance of sensation that may manifest as anxiety or hyperarousal. The detailed representation suggests intense feelings being experienced internally rather than externally observed situations.\n",
      "\n",
      "======================================================================\n",
      "ê²°ê³¼ ì €ì¥\n",
      "======================================================================\n",
      "âœ“ ê²°ê³¼ ì €ì¥ë¨: htp_interpretations.json\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "def load_htp_model(model_path=\"./htp_lora_model\", base_model_name=None):\n",
    "    \"\"\"\n",
    "    HTP LoRA ëª¨ë¸ ë¡œë“œ\n",
    "    \"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"ğŸš€ HTP ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # CUDA í™•ì¸\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"âœ“ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"âœ“ GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\\n\")\n",
    "    else:\n",
    "        print(\"âš ï¸ CPU ëª¨ë“œ (ë§¤ìš° ëŠë¦´ ìˆ˜ ìˆìŒ)\\n\")\n",
    "    \n",
    "    # ë² ì´ìŠ¤ ëª¨ë¸ ìë™ íƒì§€\n",
    "    if base_model_name is None:\n",
    "        config_path = os.path.join(model_path, \"adapter_config.json\")\n",
    "        if os.path.exists(config_path):\n",
    "            with open(config_path, 'r') as f:\n",
    "                config = json.load(f)\n",
    "                base_model_name = config.get(\"base_model_name_or_path\", \"Qwen/Qwen2.5-1.5B-Instruct\")\n",
    "        else:\n",
    "            base_model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "    \n",
    "    print(f\"ë² ì´ìŠ¤ ëª¨ë¸: {base_model_name}\")\n",
    "    \n",
    "    try:\n",
    "        # ëª¨ë¸ ë¡œë“œ\n",
    "        model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            device_map=\"auto\",\n",
    "            dtype=torch.float16,  # torch_dtype ëŒ€ì‹  dtype ì‚¬ìš©\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # LoRA ë³‘í•©\n",
    "        print(\"LoRA ê°€ì¤‘ì¹˜ ë³‘í•© ì¤‘...\")\n",
    "        model = model.merge_and_unload()\n",
    "        \n",
    "        # í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        print(\"âœ“ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\\n\")\n",
    "        return model, tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì—ëŸ¬: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def format_htp_prompt(htp_description, lang=\"ko\"):\n",
    "    \"\"\"\n",
    "    HTP í•´ì„ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "    \n",
    "    Args:\n",
    "        htp_description: HTP ê·¸ë¦¼ íŠ¹ì§• ì„¤ëª…\n",
    "        lang: 'ko' (í•œêµ­ì–´) ë˜ëŠ” 'en' (ì˜ì–´)\n",
    "    \"\"\"\n",
    "    if lang == \"ko\":\n",
    "        system_msg = \"\"\"ë‹¹ì‹ ì€ HTP(ì§‘-ë‚˜ë¬´-ì‚¬ëŒ) ê·¸ë¦¼ê²€ì‚¬ í•´ì„ ì „ë¬¸ ì‹¬ë¦¬í•™ìì…ë‹ˆë‹¤.\n",
    "ì£¼ì–´ì§„ ê·¸ë¦¼ íŠ¹ì§•ì„ ë°”íƒ•ìœ¼ë¡œ ì‹¬ë¦¬í•™ì  í•´ì„ì„ ì œê³µí•˜ì„¸ìš”.\n",
    "í•´ì„ì€ ëª…í™•í•˜ê³  ì „ë¬¸ì ì´ì–´ì•¼ í•˜ë©°, ê°€ëŠ¥í•œ ì‹¬ë¦¬ì  ì˜ë¯¸ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"<|im_start|>system\n",
    "{system_msg}<|im_end|>\n",
    "<|im_start|>user\n",
    "ë‹¤ìŒ HTP ê·¸ë¦¼ íŠ¹ì§•ì„ í•´ì„í•´ì£¼ì„¸ìš”:\n",
    "{htp_description}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "    else:\n",
    "        system_msg = \"\"\"You are a professional psychologist specialized in HTP (House-Tree-Person) test interpretation.\n",
    "Provide psychological interpretation based on the given drawing features.\n",
    "Your interpretation should be clear, professional, and explain possible psychological meanings.\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"<|im_start|>system\n",
    "{system_msg}<|im_end|>\n",
    "<|im_start|>user\n",
    "Please interpret the following HTP drawing features:\n",
    "{htp_description}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def generate_htp_interpretation(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    htp_description,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    lang=\"ko\",\n",
    "    verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    HTP í•´ì„ ìƒì„±\n",
    "    \n",
    "    Args:\n",
    "        model: ë¡œë“œëœ ëª¨ë¸\n",
    "        tokenizer: í† í¬ë‚˜ì´ì €\n",
    "        htp_description: HTP ê·¸ë¦¼ íŠ¹ì§• ì„¤ëª…\n",
    "        max_new_tokens: ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜\n",
    "        temperature: ìƒì„± ë‹¤ì–‘ì„± (ë†’ì„ìˆ˜ë¡ ì°½ì˜ì )\n",
    "        top_p: nucleus sampling\n",
    "        lang: 'ko' ë˜ëŠ” 'en'\n",
    "        verbose: ì§„í–‰ ìƒí™© ì¶œë ¥ ì—¬ë¶€\n",
    "    \n",
    "    Returns:\n",
    "        str: ìƒì„±ëœ í•´ì„\n",
    "    \"\"\"\n",
    "    # í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "    prompt = format_htp_prompt(htp_description, lang=lang)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\" * 70)\n",
    "        print(\"ğŸ“ HTP í•´ì„ ìƒì„± ì¤‘...\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"ì…ë ¥: {htp_description}\\n\")\n",
    "    \n",
    "    # í† í°í™”\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    # GPUë¡œ ì´ë™\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    input_length = inputs['input_ids'].shape[1]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"ì…ë ¥ í† í° ìˆ˜: {input_length}\")\n",
    "        print(f\"ìƒì„± ì‹œì‘... (ìµœëŒ€ {max_new_tokens} í† í°)\")\n",
    "    \n",
    "    # ìƒì„±\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.2,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    # ë””ì½”ë”© (ì…ë ¥ ì œì™¸)\n",
    "    generated_tokens = outputs[0][input_length:]\n",
    "    interpretation = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"âœ“ ìƒì„± ì™„ë£Œ! ({elapsed:.2f}ì´ˆ)\")\n",
    "        print(f\"ìƒì„±ëœ í† í° ìˆ˜: {len(generated_tokens)}\")\n",
    "        print(f\"ì†ë„: {len(generated_tokens)/elapsed:.1f} tokens/sec\")\n",
    "        print(\"=\" * 70)\n",
    "    \n",
    "    return interpretation.strip()\n",
    "\n",
    "def batch_interpret(model, tokenizer, htp_list, **kwargs):\n",
    "    \"\"\"\n",
    "    ì—¬ëŸ¬ HTP ì„¤ëª…ì„ í•œë²ˆì— í•´ì„\n",
    "    \n",
    "    Args:\n",
    "        model: ëª¨ë¸\n",
    "        tokenizer: í† í¬ë‚˜ì´ì €\n",
    "        htp_list: HTP ì„¤ëª… ë¦¬ìŠ¤íŠ¸\n",
    "        **kwargs: generate_htp_interpretationì— ì „ë‹¬í•  ì¶”ê°€ ì¸ì\n",
    "    \n",
    "    Returns:\n",
    "        list: í•´ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, htp_desc in enumerate(htp_list, 1):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"[{i}/{len(htp_list)}] í•´ì„ ì¤‘...\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        interpretation = generate_htp_interpretation(\n",
    "            model, tokenizer, htp_desc, **kwargs\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            'input': htp_desc,\n",
    "            'interpretation': interpretation\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nì…ë ¥: {htp_desc}\")\n",
    "        print(f\"í•´ì„: {interpretation}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # 1. ëª¨ë¸ ë¡œë“œ\n",
    "        model, tokenizer = load_htp_model(\"./htp_lora_model\")\n",
    "        \n",
    "        \n",
    "        # 3. ë°°ì¹˜ í•´ì„ í…ŒìŠ¤íŠ¸\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ë°°ì¹˜ í•´ì„ í…ŒìŠ¤íŠ¸\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        test_cases = [\n",
    "            \"The tree is large with many branches and deeply drawn roots\",\n",
    "            \"The house is small with no doors or windows, and no smoke from the chimney\",\n",
    "            \"The person's hands are hidden behind their back and the face has no expression\",\n",
    "            \"The tree trunk has holes and the branches are drooping downward\",\n",
    "            \"The house has an excessively large roof with many complex decorations\",\n",
    "            \"The tree is drawn in the corner with very thin trunk and sparse leaves\",\n",
    "            \"The person is drawn very small at the bottom of the page with no facial features\",\n",
    "            \"The house has heavily shaded walls and windows with bars\",\n",
    "            \"The tree has broken branches and appears to be leaning to one side\",\n",
    "            \"The person has unusually large hands and feet with exaggerated details\"\n",
    "        ]\n",
    "        \n",
    "        results = batch_interpret(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            test_cases,\n",
    "            max_new_tokens=150,\n",
    "            temperature=0.7,\n",
    "            lang=\"en\",  # ì˜ì–´ë¡œ ë³€ê²½\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # 4. ê²°ê³¼ ì €ì¥ (ì„ íƒì‚¬í•­)\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ê²°ê³¼ ì €ì¥\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        output_file = \"htp_interpretations.json\"\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"âœ“ ê²°ê³¼ ì €ì¥ë¨: {output_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ ì—ëŸ¬ ë°œìƒ: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eb1b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ ë ˆí¬ì§€í† ë¦¬ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
      "\n",
      "ì—…ë¡œë“œ ì‹œì‘...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8876dd1346f644b4ae74380f4ebe2f4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1f5320a6fe944d88527315454019cf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì—…ë¡œë“œ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "\n",
    "# âœ… ì˜¬ë°”ë¥¸ ë°©ë²• 1: í† í°ì„ ì§ì ‘ ì…ë ¥\n",
    "token = \"your_huggingface_token_here\"  # ì—¬ê¸°ì— ì‹¤ì œ í† í° ì…ë ¥\n",
    "\n",
    "# ë˜ëŠ”\n",
    "# âœ… ì˜¬ë°”ë¥¸ ë°©ë²• 2: í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸° (í™˜ê²½ ë³€ìˆ˜ ì´ë¦„ì€ HF_TOKEN ê°™ì€ ê²ƒ)\n",
    "# token = os.getenv(\"HF_TOKEN\")  # ì‹œìŠ¤í…œ í™˜ê²½ ë³€ìˆ˜ì— HF_TOKEN=your_token ì„¤ì • í•„ìš”\n",
    "\n",
    "api = HfApi(token=token)\n",
    "\n",
    "# ë¨¼ì € ë ˆí¬ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ ìƒì„±\n",
    "try:\n",
    "    api.repo_info(repo_id=\"helena29/Qwen2.5_LoRA_for_HTP\", repo_type=\"model\")\n",
    "    print(\"âœ“ ë ˆí¬ì§€í† ë¦¬ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.\")\n",
    "except:\n",
    "    print(\"ë ˆí¬ì§€í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒì„± ì¤‘...\")\n",
    "    api.create_repo(\n",
    "        repo_id=\"helena29/Qwen2.5_LoRA_for_HTP\",\n",
    "        repo_type=\"model\",\n",
    "        private=False  # Trueë¡œ ì„¤ì •í•˜ë©´ ë¹„ê³µê°œ\n",
    "    )\n",
    "    print(\"âœ“ ë ˆí¬ì§€í† ë¦¬ ìƒì„± ì™„ë£Œ!\")\n",
    "\n",
    "# LoRA ëª¨ë¸ í´ë”ë§Œ ì—…ë¡œë“œ (ì „ì²´ LoRa í´ë”ê°€ ì•„ë‹Œ ëª¨ë¸ í´ë”ë§Œ)\n",
    "print(\"\\nì—…ë¡œë“œ ì‹œì‘...\")\n",
    "api.upload_folder(\n",
    "    folder_path=r\"C:\\Users\\helen\\Desktop\\kt cloud tech up\\basic_project\\models\\LoRa\\htp_lora_model\",  # âœ… ìˆ˜ì •: ëª¨ë¸ í´ë”ë§Œ ì—…ë¡œë“œ\n",
    "    repo_id=\"helena29/Qwen2.5_LoRA_for_HTP\",\n",
    "    repo_type=\"model\",\n",
    ")\n",
    "\n",
    "print(\"âœ… ì—…ë¡œë“œ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8514f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47567c25",
   "metadata": {},
   "source": [
    "# ğŸ”„ ì „ì²´ ëª¨ë¸ ì—…ë¡œë“œ (Base Model + LoRA ë³‘í•©)\n",
    "\n",
    "í˜„ì¬ HuggingFaceì—ëŠ” LoRA ì–´ëŒ‘í„°ë§Œ ì—…ë¡œë“œëœ ìƒíƒœì…ë‹ˆë‹¤.\n",
    "ì•„ë˜ ì½”ë“œëŠ” Base Modelê³¼ LoRAë¥¼ ë³‘í•©í•œ ì „ì²´ ëª¨ë¸ì„ ì—…ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì£¼ì˜ì‚¬í•­:**\n",
    "- ì „ì²´ ëª¨ë¸ì€ ì•½ 3GB ì •ë„ì˜ í¬ê¸°ì…ë‹ˆë‹¤\n",
    "- ì—…ë¡œë“œ ì‹œê°„ì´ ì–´ëŒ‘í„°ë§Œ ì˜¬ë¦´ ë•Œë³´ë‹¤ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤\n",
    "- ì‚¬ìš©ìëŠ” ì´ ëª¨ë¸ì„ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ (LoRA ë¡œë”© ë¶ˆí•„ìš”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "236c4f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ”„ ì „ì²´ ëª¨ë¸ ìƒì„± ë° ì—…ë¡œë“œ\n",
      "======================================================================\n",
      "\n",
      "[1/4] LoRA ëª¨ë¸ ë¡œë“œ ë° Base Modelê³¼ ë³‘í•© ì¤‘...\n",
      "----------------------------------------------------------------------\n",
      "LoRA ëª¨ë¸ ê²½ë¡œ: ./htp_lora_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Exception in thread Thread-4 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\subprocess.py\", line 1515, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "  File \"c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\codecs.py\", line 322, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc0 in position 6: invalid start byte\n",
      "Exception in thread Thread-4 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\subprocess.py\", line 1515, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "  File \"c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\codecs.py\", line 322, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc0 in position 6: invalid start byte\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ LoRA ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n",
      "LoRA ê°€ì¤‘ì¹˜ë¥¼ Base Modelì— ë³‘í•© ì¤‘...\n",
      "âœ“ ë³‘í•© ì™„ë£Œ!\n",
      "âœ“ í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ\n",
      "\n",
      "[2/4] ë³‘í•©ëœ ì „ì²´ ëª¨ë¸ì„ ë¡œì»¬ì— ì €ì¥ ì¤‘...\n",
      "ì €ì¥ ê²½ë¡œ: ./htp_merged_full_model\n",
      "----------------------------------------------------------------------\n",
      "âœ“ í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ\n",
      "\n",
      "[2/4] ë³‘í•©ëœ ì „ì²´ ëª¨ë¸ì„ ë¡œì»¬ì— ì €ì¥ ì¤‘...\n",
      "ì €ì¥ ê²½ë¡œ: ./htp_merged_full_model\n",
      "----------------------------------------------------------------------\n",
      "âœ“ ë¡œì»¬ ì €ì¥ ì™„ë£Œ!\n",
      "ì €ì¥ëœ íŒŒì¼ ìˆ˜: 12\n",
      "  - model-00001-of-00002.safetensors: 1.84 GB\n",
      "âœ“ ë¡œì»¬ ì €ì¥ ì™„ë£Œ!\n",
      "ì €ì¥ëœ íŒŒì¼ ìˆ˜: 12\n",
      "  - model-00001-of-00002.safetensors: 1.84 GB\n",
      "  - model-00002-of-00002.safetensors: 1.04 GB\n",
      "  - tokenizer.json: 0.01 GB\n",
      "ì „ì²´ í¬ê¸°: 2.89 GB\n",
      "  - model-00002-of-00002.safetensors: 1.04 GB\n",
      "  - tokenizer.json: 0.01 GB\n",
      "ì „ì²´ í¬ê¸°: 2.89 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "\n",
    "# ========== ì„¤ì • ==========\n",
    "LORA_MODEL_PATH = \"./htp_lora_model\"  # ë¡œì»¬ LoRA ëª¨ë¸ ê²½ë¡œ\n",
    "BASE_MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"  # Base ëª¨ë¸\n",
    "MERGED_MODEL_PATH = \"./htp_merged_full_model\"  # ë³‘í•©ëœ ëª¨ë¸ ì €ì¥ ê²½ë¡œ\n",
    "HF_REPO_NAME = \"helena29/Qwen2.5_LoRA_for_HTP_Full\"  # HuggingFace ë ˆí¬ ì´ë¦„ (ìƒˆë¡œìš´ ë ˆí¬)\n",
    "\n",
    "# HuggingFace í† í° (ì—¬ê¸°ì— ì‹¤ì œ í† í° ì…ë ¥)\n",
    "HF_TOKEN = \"your_huggingface_token_here\"  # âš ï¸ ì‹¤ì œ í† í°ìœ¼ë¡œ ë³€ê²½ í•„ìš”!\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ”„ ì „ì²´ ëª¨ë¸ ìƒì„± ë° ì—…ë¡œë“œ\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ========== 1ë‹¨ê³„: LoRA ëª¨ë¸ ë¡œë“œ ë° ë³‘í•© ==========\n",
    "print(\"\\n[1/4] LoRA ëª¨ë¸ ë¡œë“œ ë° Base Modelê³¼ ë³‘í•© ì¤‘...\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "try:\n",
    "    # LoRA ëª¨ë¸ ë¡œë“œ\n",
    "    print(f\"LoRA ëª¨ë¸ ê²½ë¡œ: {LORA_MODEL_PATH}\")\n",
    "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "        LORA_MODEL_PATH,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    print(\"âœ“ LoRA ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
    "    \n",
    "    # LoRAë¥¼ Base Modelì— ë³‘í•©\n",
    "    print(\"LoRA ê°€ì¤‘ì¹˜ë¥¼ Base Modelì— ë³‘í•© ì¤‘...\")\n",
    "    merged_model = model.merge_and_unload()\n",
    "    print(\"âœ“ ë³‘í•© ì™„ë£Œ!\")\n",
    "    \n",
    "    # í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"âœ“ í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì—ëŸ¬: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# ========== 2ë‹¨ê³„: ë³‘í•©ëœ ëª¨ë¸ ë¡œì»¬ ì €ì¥ ==========\n",
    "print(f\"\\n[2/4] ë³‘í•©ëœ ì „ì²´ ëª¨ë¸ì„ ë¡œì»¬ì— ì €ì¥ ì¤‘...\")\n",
    "print(f\"ì €ì¥ ê²½ë¡œ: {MERGED_MODEL_PATH}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "try:\n",
    "    os.makedirs(MERGED_MODEL_PATH, exist_ok=True)\n",
    "    \n",
    "    # ëª¨ë¸ ì €ì¥\n",
    "    merged_model.save_pretrained(\n",
    "        MERGED_MODEL_PATH,\n",
    "        safe_serialization=True,  # safetensors í˜•ì‹ìœ¼ë¡œ ì €ì¥\n",
    "        max_shard_size=\"2GB\"  # ìƒ¤ë“œ í¬ê¸° ì œí•œ\n",
    "    )\n",
    "    \n",
    "    # í† í¬ë‚˜ì´ì € ì €ì¥\n",
    "    tokenizer.save_pretrained(MERGED_MODEL_PATH)\n",
    "    \n",
    "    print(\"âœ“ ë¡œì»¬ ì €ì¥ ì™„ë£Œ!\")\n",
    "    \n",
    "    # ì €ì¥ëœ íŒŒì¼ í™•ì¸\n",
    "    saved_files = os.listdir(MERGED_MODEL_PATH)\n",
    "    print(f\"ì €ì¥ëœ íŒŒì¼ ìˆ˜: {len(saved_files)}\")\n",
    "    \n",
    "    # ëª¨ë¸ íŒŒì¼ í¬ê¸° í™•ì¸\n",
    "    total_size = 0\n",
    "    for file in saved_files:\n",
    "        file_path = os.path.join(MERGED_MODEL_PATH, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            size = os.path.getsize(file_path) / (1024**3)  # GB\n",
    "            total_size += size\n",
    "            if size > 0.01:  # 10MB ì´ìƒì¸ íŒŒì¼ë§Œ ì¶œë ¥\n",
    "                print(f\"  - {file}: {size:.2f} GB\")\n",
    "    \n",
    "    print(f\"ì „ì²´ í¬ê¸°: {total_size:.2f} GB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì €ì¥ ì‹¤íŒ¨: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82816de9",
   "metadata": {},
   "source": [
    "## ğŸ“Œ ì¶”ê°€ ì˜µì…˜: ê¸°ì¡´ ë ˆí¬ì— ë®ì–´ì“°ê¸°\n",
    "\n",
    "ë§Œì•½ ìƒˆ ë ˆí¬ê°€ ì•„ë‹Œ ê¸°ì¡´ ë ˆí¬(`helena29/Qwen2.5_LoRA_for_HTP`)ì— ì „ì²´ ëª¨ë¸ì„ ë®ì–´ì“°ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì½”ë“œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc87832a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ ê¸°ì¡´ ë ˆí¬ì— ì „ì²´ ëª¨ë¸ ë®ì–´ì“°ê¸° ëª¨ë“œ\n",
      "======================================================================\n",
      "[1/3] LoRA ëª¨ë¸ ë³‘í•© ì¤‘...\n",
      "âœ“ ë³‘í•© ì™„ë£Œ\n",
      "\n",
      "[2/3] ë¡œì»¬ ì €ì¥ ì¤‘: ./htp_merged_full_model\n",
      "âœ“ ë³‘í•© ì™„ë£Œ\n",
      "\n",
      "[2/3] ë¡œì»¬ ì €ì¥ ì¤‘: ./htp_merged_full_model\n",
      "âœ“ ì €ì¥ ì™„ë£Œ\n",
      "\n",
      "[3/3] ê¸°ì¡´ ë ˆí¬ì— ì—…ë¡œë“œ ì¤‘: helena29/Qwen2.5_LoRA_for_HTP\n",
      "âš ï¸ ê¸°ì¡´ íŒŒì¼ë“¤ì„ ì‚­ì œí•˜ê³  ìƒˆ íŒŒì¼ë¡œ êµì²´í•©ë‹ˆë‹¤...\n",
      "âœ“ ì €ì¥ ì™„ë£Œ\n",
      "\n",
      "[3/3] ê¸°ì¡´ ë ˆí¬ì— ì—…ë¡œë“œ ì¤‘: helena29/Qwen2.5_LoRA_for_HTP\n",
      "âš ï¸ ê¸°ì¡´ íŒŒì¼ë“¤ì„ ì‚­ì œí•˜ê³  ìƒˆ íŒŒì¼ë¡œ êµì²´í•©ë‹ˆë‹¤...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fabc3d7d103942939bc5d7b5187d3fc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b40e485da457482cba5fdde8d27b5539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ ì—…ë¡œë“œ ì™„ë£Œ!\n",
      "âœ“ URL: https://huggingface.co/helena29/Qwen2.5_LoRA_for_HTP\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "\n",
    "# ========== ì„¤ì • ==========\n",
    "LORA_MODEL_PATH = \"./htp_lora_model\"  # ë¡œì»¬ LoRA ëª¨ë¸ ê²½ë¡œ\n",
    "BASE_MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"  # Base ëª¨ë¸\n",
    "MERGED_MODEL_PATH = \"./htp_merged_full_model\"  # ë³‘í•©ëœ ëª¨ë¸ ì €ì¥ ê²½ë¡œ\n",
    "HF_REPO_NAME = \"helena29/Qwen2.5_LoRA_for_HTP_Full\"  # HuggingFace ë ˆí¬ ì´ë¦„ (ìƒˆë¡œìš´ ë ˆí¬)\n",
    "\n",
    "# HuggingFace í† í° (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°)\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")  # í™˜ê²½ ë³€ìˆ˜ì—ì„œ í† í° ê°€ì ¸ì˜¤ê¸°\n",
    "if not HF_TOKEN:\n",
    "    raise ValueError(\"HF_TOKEN í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ì„¤ì •í•´ì£¼ì„¸ìš”.\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ”„ ì „ì²´ ëª¨ë¸ ìƒì„± ë° ì—…ë¡œë“œ\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ========== 1ë‹¨ê³„: LoRA ëª¨ë¸ ë¡œë“œ ë° ë³‘í•© ==========\n",
    "print(\"\\n[1/4] LoRA ëª¨ë¸ ë¡œë“œ ë° Base Modelê³¼ ë³‘í•© ì¤‘...\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "try:\n",
    "    # LoRA ëª¨ë¸ ë¡œë“œ\n",
    "    print(f\"LoRA ëª¨ë¸ ê²½ë¡œ: {LORA_MODEL_PATH}\")\n",
    "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "        LORA_MODEL_PATH,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… LoRA ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
    "    print(f\"Base ëª¨ë¸: {BASE_MODEL_NAME}\")\n",
    "    \n",
    "    # ë³‘í•©\n",
    "    print(\"\\në³‘í•© ì‹œì‘...\")\n",
    "    merged_model = model.merge_and_unload()\n",
    "    print(\"âœ… ë³‘í•© ì™„ë£Œ!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154fb1b9",
   "metadata": {},
   "source": [
    "## ğŸ”§ ë¬¸ì œ í•´ê²°: adapter_config.json íŒŒì¼ ì—…ë¡œë“œ\n",
    "\n",
    "HuggingFaceì—ì„œ ë‹¤ìš´ë¡œë“œ ì‹œ `adapter_config.json` íŒŒì¼ì´ ì—†ë‹¤ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ ì•„ë˜ ì½”ë“œë¡œ ëˆ„ë½ëœ íŒŒì¼ë“¤ì„ ë‹¤ì‹œ ì—…ë¡œë“œí•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff36369f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
