"""
HTP RAG System FastAPI Server

ë¡œì»¬ RAG ì‹œìŠ¤í…œì„ ì›¹ APIë¡œ ì œê³µí•˜ëŠ” FastAPI ì„œë²„
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Optional
import torch
from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, BlipProcessor, BlipForConditionalGeneration
from langchain_community.vectorstores import Chroma
from langchain.embeddings.base import Embeddings
from sentence_transformers import CrossEncoder
import json
from datetime import datetime
import asyncio
import base64
from io import BytesIO
from PIL import Image
import openai
import os

# ============================================
# 1. ì„¤ì • ë° ì „ì—­ ë³€ìˆ˜
# ============================================

app = FastAPI(title="HTP RAG API", version="1.0.0")

# CORS ì„¤ì • (React ì•±ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•˜ë„ë¡)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # í”„ë¡œë•ì…˜ì—ì„œëŠ” specific originsìœ¼ë¡œ ë³€ê²½
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì „ì—­ ë³€ìˆ˜
device = "cuda" if torch.cuda.is_available() else "cpu"
rag_system = None
captioning_model = None
sessions = {}  # ì„¸ì…˜ë³„ ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬

# OpenAI API í‚¤ ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°)
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "your-api-key-here")
openai.api_key = OPENAI_API_KEY

# ============================================
# 2. Pydantic ëª¨ë¸ (ìš”ì²­/ì‘ë‹µ ìŠ¤í‚¤ë§ˆ)
# ============================================

class ChatRequest(BaseModel):
    message: str
    session_id: Optional[str] = "default"

class ImageInterpretRequest(BaseModel):
    image: str  # base64 encoded image
    drawing_type: str  # "house", "tree", or "person"

class MultipleImageRequest(BaseModel):
    house: Optional[str] = None  # base64 encoded image
    tree: Optional[str] = None   # base64 encoded image
    person: Optional[str] = None  # base64 encoded image

class ImageInterpretResponse(BaseModel):
    caption: str
    interpretation: str
    rewritten_queries: List[str]
    source_documents: List[Dict]

class MultipleImageResponse(BaseModel):
    house: Optional[ImageInterpretResponse] = None
    tree: Optional[ImageInterpretResponse] = None
    person: Optional[ImageInterpretResponse] = None
    combined_interpretation: str

class ChatResponse(BaseModel):
    response: str
    rewritten_queries: List[str]
    source_documents: List[Dict]
    session_id: str

class ResetRequest(BaseModel):
    session_id: Optional[str] = "default"

# ============================================
# 3. ì´ë¯¸ì§€ ìº¡ì…˜ ìƒì„± í´ë˜ìŠ¤
# ============================================

class ImageCaptioner:
    def __init__(self, model_name="Salesforce/blip-image-captioning-large"):
        print(f"âœ… ì´ë¯¸ì§€ ìº¡ì…˜ ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.processor = BlipProcessor.from_pretrained(model_name)
        self.model = BlipForConditionalGeneration.from_pretrained(model_name).to(self.device)
        print(f"âœ… ì´ë¯¸ì§€ ìº¡ì…˜ ëª¨ë¸ ë¡œë”© ì™„ë£Œ! Device: {self.device}")
    
    def generate_caption(self, image_base64: str, drawing_type: str) -> str:
        """
        Base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ì—ì„œ ìº¡ì…˜ ìƒì„±
        
        Args:
            image_base64: base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ ë¬¸ìì—´
            drawing_type: "house", "tree", "person" ì¤‘ í•˜ë‚˜
        
        Returns:
            str: ìƒì„±ëœ ìº¡ì…˜
        """
        try:
            # Base64 ë””ì½”ë”©
            if ',' in image_base64:
                image_base64 = image_base64.split(',')[1]
            
            image_data = base64.b64decode(image_base64)
            image = Image.open(BytesIO(image_data)).convert('RGB')
            
            # ê·¸ë¦¼ ìœ í˜•ì— ë§ëŠ” í”„ë¡¬í”„íŠ¸ ì„¤ì •
            prompts = {
                "house": "A detailed description of this house drawing, including size, windows, doors, chimney, roof, and overall structure:",
                "tree": "A detailed description of this tree drawing, including trunk, branches, leaves, roots, and overall shape:",
                "person": "A detailed description of this person drawing, including body parts, posture, facial features, and overall appearance:"
            }
            
            prompt = prompts.get(drawing_type.lower(), "A description of this drawing:")
            
            # ìº¡ì…˜ ìƒì„±
            inputs = self.processor(image, prompt, return_tensors="pt").to(self.device)
            
            with torch.no_grad():
                output = self.model.generate(
                    **inputs,
                    max_new_tokens=100,
                    num_beams=5,
                    temperature=1.0
                )
            
            caption = self.processor.decode(output[0], skip_special_tokens=True)
            
            # ê·¸ë¦¼ ìœ í˜• ì •ë³´ ì¶”ê°€
            full_caption = f"HTP {drawing_type.upper()} drawing: {caption}"
            
            print(f"ìƒì„±ëœ ìº¡ì…˜ ({drawing_type}): {full_caption}")
            return full_caption
            
        except Exception as e:
            print(f"ìº¡ì…˜ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return f"HTP {drawing_type} drawing with unclear features"

# ============================================
# 4. ì„ë² ë”© ë˜í¼ í´ë˜ìŠ¤
# ============================================

class MyEmbeddings(Embeddings):
    def __init__(self, model, tokenizer, device="cpu"):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device

    def embed_documents(self, texts):
        return [self.embed_query(t) for t in texts]

    def embed_query(self, text):
        inputs = self.tokenizer(text, return_tensors="pt", truncation=True, padding=True).to(self.device)
        with torch.no_grad():
            emb = self.model(**inputs).last_hidden_state[:, 0, :]
            emb = emb / emb.norm(dim=1, keepdim=True)
        return emb.cpu().numpy()[0]

# ============================================
# 5. ì¿¼ë¦¬ ì¬ì‘ì„±ê¸° (OpenAI API ì‚¬ìš©)
# ============================================

class AdvancedQueryRewriter:
    def __init__(self, api_key=None):
        print(f"âœ… OpenAI ê¸°ë°˜ ì¿¼ë¦¬ ì¬ì‘ì„±ê¸° ì´ˆê¸°í™”")
        self.api_key = api_key or OPENAI_API_KEY
        openai.api_key = self.api_key
        print(f"âœ… OpenAI API ì„¤ì • ì™„ë£Œ!")

        self.template = """You are an assistant that regenerates search queries based on the user's previous conversations and questions.

# Instructions
1. Reference all previous queries/retrieved documents/answers in the history below to generate more accurate search queries.
2. If the current question is ambiguous or incomplete, use the history to reconstruct a contextually complete query.
3. If there is no history or it's not relevant, use only the current question.
4. Always generate clear and search-appropriate queries.
5. The output should contain only the regenerated query strings. Do not include additional explanations or comments.
6. If the current sentence contains multiple attributes, separate each into individual queries.
7. Each query should be complete and clear enough to be independently searchable in a vector DB.
8. When combined, the separated queries should represent the meaning of the original query.

# Input
Full conversation history: {history_text}
Current question: {current_query}

# Output Format
You must output in the following JSON format:
{{
    "queries": ["query1", "query2", ...]
}}

Example:
If the current question is "What about Seoul? And restaurants?" and the previous conversation was "Recommend tourist spots in Korea",
{{
    "queries": ["Recommend tourist spots in Seoul", "Recommend restaurants in Seoul"]
}}

Single query case:
{{
    "queries": ["Recommend tourist spots in Korea"]
}}
"""

    def rewrite_query(self, history_text: str, current_query: str) -> List[str]:
        """
        OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì¿¼ë¦¬ ì¬ì‘ì„±
        """
        if not history_text.strip():
            history_text = "No previous conversation"

        try:
            prompt = self.template.format(
                history_text=history_text,
                current_query=current_query
            )
            
            # OpenAI API í˜¸ì¶œ
            response = openai.ChatCompletion.create(
                model="gpt-4o-mini",  # ë˜ëŠ” "gpt-3.5-turbo"
                messages=[
                    {"role": "system", "content": "You are a helpful assistant for query rewriting."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=500
            )
            
            response_text = response.choices[0].message.content
            
            # JSON íŒŒì‹±
            try:
                import re
                json_match = re.search(r'\{[^{}]*"queries"[^{}]*\}', response_text, re.DOTALL)
                if json_match:
                    response_json = json.loads(json_match.group())
                else:
                    response_json = json.loads(response_text)
                
                if "queries" in response_json and isinstance(response_json["queries"], list):
                    return response_json["queries"]
                else:
                    return [current_query]
            except Exception as e:
                print(f"JSON parsing error: {str(e)}")
                return [current_query]
                
        except Exception as e:
            print(f"Error during query rewriting: {str(e)}")
            return [current_query]

# ============================================
# 6. ë©€í‹° ì¿¼ë¦¬ ë¦¬íŠ¸ë¦¬ë²„
# ============================================

class MultiQueryRetriever:
    def __init__(self, vectorstore, query_rewriter, **kwargs):
        self.vectorstore = vectorstore
        self.query_rewriter = query_rewriter
        self.history = []

    def build_history_text(self) -> str:
        text = ""
        for h in self.history:
            text += f"[QUESTION]\n{h['user_query']}\n"
            text += f"[REWRITTEN QUERIES]\n{h['rewritten_queries']}\n"
            text += "[RETRIEVED DOCS]\n"
            for d in h["retrieved_docs"]:
                text += f"- {d['content']}\n"
            text += f"[ANSWER]\n{h['final_answer']}\n"
            text += "-"*40 + "\n"
        return text

    def retrieve(self, query: str, num_docs=3):
        history_text = self.build_history_text()
        rewritten_queries = self.query_rewriter.rewrite_query(
            history_text=history_text,
            current_query=query
        )

        print(f"ì›ë˜ ì¿¼ë¦¬: {query}")
        print(f"ì¬ìƒì„±ëœ ì¿¼ë¦¬ë“¤: {rewritten_queries}")

        all_docs = []
        seen_contents = set()

        for idx, rewritten_query in enumerate(rewritten_queries):
            print(f"ì¿¼ë¦¬ {idx+1} : {rewritten_query}")
            docs = self.vectorstore.similarity_search(rewritten_query, k=num_docs)

            for doc in docs:
                if doc.page_content not in seen_contents:
                    seen_contents.add(doc.page_content)
                    if not hasattr(doc, "metadata") or doc.metadata is None:
                        doc.metadata = {}
                    doc.metadata['query'] = rewritten_query
                    all_docs.append(doc)

        print(f"ì´ {len(all_docs)}ê°œì˜ ê³ ìœ  ë¬¸ì„œë¥¼ ê²€ìƒ‰í–ˆìŠµë‹ˆë‹¤.")
        return all_docs, rewritten_queries

# ============================================
# 7. RAG ì‹œìŠ¤í…œ
# ============================================

class AdvancedConversationalRAG:
    def __init__(self, vectorstore, llm_model_name="helena29/Qwen2.5_LoRA_for_HTP"):
        self.history = []
        self.query_rewriter = AdvancedQueryRewriter()  # OpenAI ì‚¬ìš©
        self.retriever = MultiQueryRetriever(vectorstore=vectorstore, query_rewriter=self.query_rewriter)
        
        # ë‹µë³€ ìƒì„±ìš© ë¡œì»¬ LLM ë¡œë“œ
        print(f"âœ… ë‹µë³€ ìƒì„± ëª¨ë¸ ë¡œë”© ì¤‘: {llm_model_name}")
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        try:
            from peft import AutoPeftModelForCausalLM
            self.llm = AutoPeftModelForCausalLM.from_pretrained(
                llm_model_name,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                trust_remote_code=True
            ).to(self.device)
            self.tokenizer = AutoTokenizer.from_pretrained(llm_model_name, trust_remote_code=True)
        except:
            base_model = "Qwen/Qwen2.5-1.5B-Instruct"
            self.tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
            self.llm = AutoModelForCausalLM.from_pretrained(
                base_model,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                trust_remote_code=True
            ).to(self.device)
        
        self.llm.eval()
        print(f"âœ… ë‹µë³€ ìƒì„± ëª¨ë¸ ë¡œë”© ì™„ë£Œ! Device: {self.device}")

        self.response_template = """You are a professional psychologist specialized in HTP (House-Tree-Person) test interpretation.
Your role is to provide clear, professional psychological interpretations based on drawing features.

User Question: {query}

Please provide your interpretation based on the following reference information:
{context}

Guidelines:
1. If the user's question contains multiple queries, address each one clearly and separately.
2. Base your answer only on the provided information. If information is insufficient, honestly state that you don't know.
3. Provide your answer in Korean language.
4. If there are original sources in the provided information, cite them appropriately.
5. Explain possible psychological meanings in a professional manner.

Answer:"""
        
    def generate_response(self, prompt: str) -> str:
        messages = [
            {"role": "system", "content": "You are a professional psychologist specialized in HTP test interpretation."},
            {"role": "user", "content": prompt}
        ]
        
        formatted_prompt = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        inputs = self.tokenizer(formatted_prompt, return_tensors="pt")
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = self.llm.generate(
                **inputs,
                max_new_tokens=1024,
                temperature=0.3,
                do_sample=True,
                top_p=0.9,
                repetition_penalty=1.1,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )
        
        response = self.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
        return response.strip()
        
    def query(self, current_query: str) -> Dict:
        docs, rewritten_queries = self.retriever.retrieve(current_query)

        if docs:
            context = "\n\n".join([f"ë¬¸ì„œ {i+1}:\n{doc.page_content}" for i, doc in enumerate(docs)])
            formatted_prompt = self.response_template.format(query=current_query, context=context)
        else:
            formatted_prompt = f"User Question: {current_query}\n\nNo documents were retrieved, but please provide an appropriate answer based on your knowledge."

        response = self.generate_response(formatted_prompt)

        record = {
            "user_query": current_query,
            "rewritten_queries": rewritten_queries,
            "retrieved_docs": [
                {"content": d.page_content, "metadata": d.metadata} for d in docs
            ],
            "final_answer": response
        }
        self.history.append(record)
        self.retriever.history.append(record)

        return {
            "query": current_query,
            "result": response,
            "rewritten_queries": rewritten_queries,
            "source_documents": docs
        }

# ============================================
# 8. ì„œë²„ ì´ˆê¸°í™” (ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰)
# ============================================

@app.on_event("startup")
async def startup_event():
    global rag_system, captioning_model
    
    print("=" * 60)
    print("ğŸš€ HTP RAG ì„œë²„ ì‹œì‘ ì¤‘...")
    print("=" * 60)
    
    try:
        # 1. ì´ë¯¸ì§€ ìº¡ì…˜ ëª¨ë¸ ë¡œë“œ
        print("\n[1/4] ì´ë¯¸ì§€ ìº¡ì…˜ ëª¨ë¸ ë¡œë“œ ì¤‘...")
        captioning_model = ImageCaptioner()
        print("âœ… ì´ë¯¸ì§€ ìº¡ì…˜ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        
        # 2. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        print("\n[2/4] ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
        embedding_model_name = "HJUNN/bge-m3b-Art-Therapy-embedding-fine-tuning"
        embed_tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)
        embed_model = AutoModel.from_pretrained(embedding_model_name).to(device)
        embeddings = MyEmbeddings(embed_model, embed_tokenizer, device=device)
        print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        
        # 3. ë²¡í„° DB ë¡œë“œ
        print("\n[3/4] ë²¡í„° DB ë¡œë“œ ì¤‘...")
        vectorstore = Chroma(
            embedding_function=embeddings,
            collection_name="htp_collection",
            persist_directory="./chroma_store"
        )
        print("âœ… ë²¡í„° DB ë¡œë“œ ì™„ë£Œ!")
        
        # 4. RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        print("\n[4/4] RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        rag_system = AdvancedConversationalRAG(vectorstore)
        print("âœ… RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
        
        print("\n" + "=" * 60)
        print("âœ… ì„œë²„ ì¤€ë¹„ ì™„ë£Œ!")
        print(f"ğŸ“ Device: {device}")
        print(f"ğŸŒ API ë¬¸ì„œ: http://localhost:8000/docs")
        print("=" * 60)
        
    except Exception as e:
        print(f"\nâŒ ì„œë²„ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        raise

# ============================================
# 9. API ì—”ë“œí¬ì¸íŠ¸
# ============================================

@app.get("/")
async def root():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return {
        "status": "running",
        "message": "HTP RAG API Server with Image Captioning",
        "device": device,
        "active_sessions": len(sessions),
        "captioning_ready": captioning_model is not None,
        "rag_ready": rag_system is not None
    }

@app.post("/interpret-image", response_model=ImageInterpretResponse)
async def interpret_image(request: ImageInterpretRequest):
    """
    ì´ë¯¸ì§€ ê¸°ë°˜ HTP í•´ì„ ì—”ë“œí¬ì¸íŠ¸
    
    1. ì´ë¯¸ì§€ â†’ ìº¡ì…˜ ìƒì„±
    2. ìº¡ì…˜ â†’ RAG ê²€ìƒ‰
    3. ê²€ìƒ‰ ê²°ê³¼ â†’ í•´ì„ ìƒì„±
    """
    if captioning_model is None:
        raise HTTPException(status_code=503, detail="Captioning model not initialized")
    
    if rag_system is None:
        raise HTTPException(status_code=503, detail="RAG system not initialized")
    
    try:
        # 1ë‹¨ê³„: ì´ë¯¸ì§€ì—ì„œ ìº¡ì…˜ ìƒì„±
        print(f"\n{'='*70}")
        print(f"[1/3] ì´ë¯¸ì§€ ìº¡ì…˜ ìƒì„± ì¤‘... (Type: {request.drawing_type})")
        print(f"{'='*70}")
        
        caption = captioning_model.generate_caption(
            request.image,
            request.drawing_type
        )
        
        print(f"ìƒì„±ëœ ìº¡ì…˜: {caption}")
        
        # 2ë‹¨ê³„: ìº¡ì…˜ ê¸°ë°˜ìœ¼ë¡œ RAG ê²€ìƒ‰ ë° í•´ì„
        print(f"\n{'='*70}")
        print(f"[2/3] RAG ê²€ìƒ‰ ë° í•´ì„ ìƒì„± ì¤‘...")
        print(f"{'='*70}")
        
        result = rag_system.query(caption)
        
        print(f"\n{'='*70}")
        print(f"[3/3] í•´ì„ ì™„ë£Œ!")
        print(f"{'='*70}")
        print(f"ì¬ì‘ì„±ëœ ì¿¼ë¦¬: {result['rewritten_queries']}")
        print(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(result['source_documents'])}")
        print(f"í•´ì„ ê¸¸ì´: {len(result['result'])} ë¬¸ì")
        
        return ImageInterpretResponse(
            caption=caption,
            interpretation=result["result"],
            rewritten_queries=result["rewritten_queries"],
            source_documents=[
                {
                    "content": doc.page_content,
                    "metadata": doc.metadata
                }
                for doc in result["source_documents"]
            ]
        )
        
    except Exception as e:
        print(f"\nâŒ í•´ì„ ì‹¤íŒ¨: {str(e)}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error processing image: {str(e)}")

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ (ì¼ë°˜)
    """
    if rag_system is None:
        raise HTTPException(status_code=503, detail="RAG system not initialized")
    
    try:
        # ì„¸ì…˜ë³„ RAG ì‹œìŠ¤í…œ ê´€ë¦¬
        session_id = request.session_id
        if session_id not in sessions:
            sessions[session_id] = {
                "created_at": datetime.now().isoformat(),
                "message_count": 0
            }
        
        # RAG ì¿¼ë¦¬ ì‹¤í–‰
        result = rag_system.query(request.message)
        
        # ì„¸ì…˜ ì •ë³´ ì—…ë°ì´íŠ¸
        sessions[session_id]["message_count"] += 1
        sessions[session_id]["last_message"] = datetime.now().isoformat()
        
        return ChatResponse(
            response=result["result"],
            rewritten_queries=result["rewritten_queries"],
            source_documents=[
                {
                    "content": doc.page_content,
                    "metadata": doc.metadata
                }
                for doc in result["source_documents"]
            ],
            session_id=session_id
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error processing request: {str(e)}")

@app.post("/reset")
async def reset_session(request: ResetRequest):
    """
    ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
    """
    session_id = request.session_id
    
    if session_id in sessions:
        del sessions[session_id]
    
    # RAG ì‹œìŠ¤í…œ íˆìŠ¤í† ë¦¬ë„ ì´ˆê¸°í™”
    if rag_system:
        rag_system.history = []
        rag_system.retriever.history = []
    
    return {
        "message": f"Session {session_id} reset successfully",
        "session_id": session_id
    }

@app.get("/sessions")
async def get_sessions():
    """
    í™œì„± ì„¸ì…˜ ëª©ë¡ ì¡°íšŒ
    """
    return {
        "active_sessions": len(sessions),
        "sessions": sessions
    }

@app.get("/history/{session_id}")
async def get_history(session_id: str):
    """
    íŠ¹ì • ì„¸ì…˜ì˜ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¡°íšŒ
    """
    if rag_system is None:
        raise HTTPException(status_code=503, detail="RAG system not initialized")
    
    return {
        "session_id": session_id,
        "history": rag_system.history
    }

@app.post("/interpret-multiple-images", response_model=MultipleImageResponse)
async def interpret_multiple_images(request: MultipleImageRequest):
    """
    ë©€í‹° ì´ë¯¸ì§€ HTP í•´ì„ ì—”ë“œí¬ì¸íŠ¸
    
    ì›¹ì—ì„œ 3ê°œì˜ ì´ë¯¸ì§€ë¥¼ í•œ ë²ˆì— ì „ì†¡í•˜ë©´ ê°ê° í•´ì„ í›„ ì¢…í•© í•´ì„ ì œê³µ
    """
    if captioning_model is None:
        raise HTTPException(status_code=503, detail="Captioning model not initialized")
    
    if rag_system is None:
        raise HTTPException(status_code=503, detail="RAG system not initialized")
    
    try:
        results = {
            "house": None,
            "tree": None,
            "person": None
        }
        
        all_interpretations = []
        
        print(f"\n{'='*70}")
        print(f"[ë©€í‹° ì´ë¯¸ì§€ í•´ì„ ì‹œì‘]")
        print(f"{'='*70}")
        
        # ê° ì´ë¯¸ì§€ íƒ€ì…ë³„ë¡œ ì²˜ë¦¬
        for img_type in ["house", "tree", "person"]:
            image_data = getattr(request, img_type)
            
            if image_data:
                print(f"\n[{img_type.upper()}] ì²˜ë¦¬ ì¤‘...")
                
                # 1. ìº¡ì…˜ ìƒì„±
                caption = captioning_model.generate_caption(image_data, img_type)
                print(f"ìº¡ì…˜: {caption}")
                
                # 2. RAG ê²€ìƒ‰ ë° í•´ì„
                result = rag_system.query(caption)
                
                results[img_type] = ImageInterpretResponse(
                    caption=caption,
                    interpretation=result["result"],
                    rewritten_queries=result["rewritten_queries"],
                    source_documents=[
                        {
                            "content": doc.page_content,
                            "metadata": doc.metadata
                        }
                        for doc in result["source_documents"]
                    ]
                )
                
                all_interpretations.append(f"[{img_type.upper()}]\n{result['result']}")
        
        # 3. ì¢…í•© í•´ì„ ìƒì„±
        if all_interpretations:
            combined_prompt = f"""ë‹¤ìŒì€ HTP ê²€ì‚¬ì˜ ê° ê·¸ë¦¼ì— ëŒ€í•œ ê°œë³„ í•´ì„ì…ë‹ˆë‹¤:

{chr(10).join(all_interpretations)}

ìœ„ ê°œë³„ í•´ì„ë“¤ì„ ì¢…í•©í•˜ì—¬ ì „ì²´ì ì¸ ì‹¬ë¦¬ ìƒíƒœë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.
- ê³µí†µì ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ” íŠ¹ì§•
- ì„¸ ê·¸ë¦¼ ê°„ì˜ ì—°ê´€ì„±
- ì¢…í•©ì ì¸ ì‹¬ë¦¬ ìƒíƒœ í‰ê°€
- ê¸ì •ì  ì¸¡ë©´ê³¼ ë°œì „ ë°©í–¥

í•œêµ­ì–´ë¡œ 4-5 ë¬¸ë‹¨ ì •ë„ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."""
            
            combined_interpretation = rag_system.generate_response(combined_prompt)
        else:
            combined_interpretation = "ì œê³µëœ ì´ë¯¸ì§€ê°€ ì—†ì–´ í•´ì„ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        print(f"\n{'='*70}")
        print(f"[ë©€í‹° ì´ë¯¸ì§€ í•´ì„ ì™„ë£Œ]")
        print(f"{'='*70}")
        
        return MultipleImageResponse(
            house=results["house"],
            tree=results["tree"],
            person=results["person"],
            combined_interpretation=combined_interpretation
        )
        
    except Exception as e:
        print(f"\nâŒ ë©€í‹° ì´ë¯¸ì§€ í•´ì„ ì‹¤íŒ¨: {str(e)}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error processing multiple images: {str(e)}")

# ============================================
# 10. ì„œë²„ ì‹¤í–‰
# ============================================

if __name__ == "__main__":
    import uvicorn
    
    print("\nğŸš€ ì„œë²„ ì‹œì‘...")
    print("ğŸ“ ì£¼ì†Œ: http://localhost:8000")
    print("ğŸ“– API ë¬¸ì„œ: http://localhost:8000/docs")
    print("\nì¢…ë£Œí•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”.\n")
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        log_level="info"
    )
