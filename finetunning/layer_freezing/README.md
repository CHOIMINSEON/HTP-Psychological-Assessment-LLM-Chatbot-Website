# HTP ì‹¬ë¦¬ê²€ì‚¬ í•´ì„ AI ëª¨ë¸

## ğŸ“‹ ëª¨ë¸ ê°œìš”

**HTP (House-Tree-Person)** ì‹¬ë¦¬ê²€ì‚¬ ì´ë¯¸ì§€ ìº¡ì…˜ì„ ì…ë ¥ë°›ì•„ ì „ë¬¸ì ì¸ ì‹¬ë¦¬í•™ì  í•´ì„ì„ ìƒì„±í•˜ëŠ” AI ëª¨ë¸ì…ë‹ˆë‹¤.

### ê¸°ë³¸ ì •ë³´
- **ë² ì´ìŠ¤ ëª¨ë¸**: Qwen/Qwen2.5-1.5B-Instruct
- **í•™ìŠµ ë°©ë²•**: Layer Freezing (26/28 ë ˆì´ì–´ ë™ê²°)
- **í•™ìŠµ íŒŒë¼ë¯¸í„°**: ì „ì²´ì˜ ì•½ 7-10%ë§Œ ì—…ë°ì´íŠ¸
- **ë°ì´í„°**: HTP ì‹¬ë¦¬ê²€ì‚¬ í•´ì„ ë°ì´í„° 1,453ê°œ ìƒ˜í”Œ
- **ì¹´í…Œê³ ë¦¬**: House, Tree, Person

---

## ğŸ§  ëª¨ë¸ í•™ìŠµ ë°©ë²• ìƒì„¸

### 1. Loss (ì†ì‹¤ í•¨ìˆ˜)
```
Loss = Cross Entropy Loss (êµì°¨ ì—”íŠ¸ë¡œí”¼)
```

**ì˜ë¯¸**:
- ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ë‹¤ìŒ í† í°ì˜ í™•ë¥  ë¶„í¬ì™€ ì‹¤ì œ ì •ë‹µ í† í° ê°„ì˜ ì°¨ì´ë¥¼ ì¸¡ì •
- Lossê°€ ë‚®ì„ìˆ˜ë¡ ëª¨ë¸ì˜ ì˜ˆì¸¡ì´ ì •ë‹µì— ê°€ê¹Œì›€

**ê³„ì‚° ë°©ì‹**:
```python
ì…ë ¥: "This suggests a strong personality"
ì •ë‹µ ë‹¤ìŒ í† í°: "with"
ëª¨ë¸ ì˜ˆì¸¡ í™•ë¥ : [
    "with": 0.85,  # ë†’ì€ í™•ë¥  â†’ ë‚®ì€ Loss
    "and": 0.10,
    "but": 0.05
]
```

### 2. í•™ìŠµ ë°ì´í„° êµ¬ì¡°
```json
{
  "instruction": "Please provide a psychological interpretation...",
  "input": "The tree is dominant and tall.",
  "output": "This suggests a strong, assertive personality...",
  "category": "tree"
}
```

### 3. í•™ìŠµ í”„ë¡œì„¸ìŠ¤

#### Step 1: ë°ì´í„° í¬ë§·íŒ…
```
System: "You are an expert psychologist..."
User: "Please interpret this caption: [ì´ë¯¸ì§€ ì„¤ëª…]"
Assistant: "[ì‹¬ë¦¬í•™ì  í•´ì„]"
```

#### Step 2: í† í°í™”
- ìµœëŒ€ ê¸¸ì´: 1024 í† í°
- ë™ì  íŒ¨ë”© (ë°°ì¹˜ë§ˆë‹¤ ìµœëŒ€ ê¸¸ì´ ì¡°ì •)

#### Step 3: Layer Freezing
```
ì´ 28ê°œ ë ˆì´ì–´:
â”œâ”€ Layer 0-25 (26ê°œ): ğŸ”’ ë™ê²° (ì—…ë°ì´íŠ¸ X)
â”œâ”€ Layer 26-27 (2ê°œ): ğŸ”“ í•™ìŠµ (ì—…ë°ì´íŠ¸ O)
â”œâ”€ Final LayerNorm: ğŸ”“ í•™ìŠµ
â””â”€ LM Head: ğŸ”“ í•™ìŠµ
```

**ì¥ì **:
- GPU ë©”ëª¨ë¦¬ ì ˆì•½ (8GB GPUì—ì„œë„ ê°€ëŠ¥)
- ë¹ ë¥¸ í•™ìŠµ ì†ë„
- ê³¼ì í•© ë°©ì§€
- ë² ì´ìŠ¤ ëª¨ë¸ì˜ ì¼ë°˜ ì§€ì‹ ë³´ì¡´

#### Step 4: í•™ìŠµ ì„¤ì •
```yaml
ë°°ì¹˜ í¬ê¸°: 2 (per device)
Gradient Accumulation: 4
ì‹¤ì§ˆì  ë°°ì¹˜ í¬ê¸°: 2 Ã— 4 = 8

í•™ìŠµë¥ : 5e-4
ìŠ¤ì¼€ì¤„ëŸ¬: Cosine (ì ì§„ì  ê°ì†Œ)
Optimizer: AdamW (weight_decay=0.01)
ì—í­: 10

ë©”ëª¨ë¦¬ ìµœì í™”:
  - BFloat16 ì •ë°€ë„
  - Gradient Checkpointing
  - 26/28 ë ˆì´ì–´ ë™ê²°
```

### 4. Loss ë¹„êµ ëŒ€ìƒ

#### Training Loss
- **ë°ì´í„°**: í•™ìŠµ ë°ì´í„° (1,307ê°œ ìƒ˜í”Œ)
- **ë³€í™”**: 0.841 â†’ 0.280 (67% ê°ì†Œ)
- **ì˜ë¯¸**: ëª¨ë¸ì´ í•™ìŠµ ë°ì´í„°ë¥¼ ì˜ í•™ìŠµí•¨

#### Validation Loss
- **ë°ì´í„°**: ê²€ì¦ ë°ì´í„° (146ê°œ ìƒ˜í”Œ)
- **ìµœì¢…ê°’**: ì•½ 1.809
- **ì˜ë¯¸**: 
  - Training Lossë³´ë‹¤ ë†’ìŒ = ì•½ê°„ì˜ ê³¼ì í•©
  - í•˜ì§€ë§Œ í° ì°¨ì´ ì•„ë‹˜ = ì¼ë°˜í™” ì„±ëŠ¥ ì–‘í˜¸
  - ìƒˆë¡œìš´ ë°ì´í„°ì—ë„ ì˜ ì‘ë™

### 5. í•™ìŠµ ê²°ê³¼
```
í•™ìŠµ ì‹œê°„: ì•½ 39ë¶„
ìµœì¢… Training Loss: 0.280
ìµœì¢… Validation Loss: 1.809
Loss ê°œì„ ë„: 67% ê°ì†Œ
ëª¨ë¸ í¬ê¸°: 2.96 GB
```

---

## ğŸš€ ì‚¬ìš© ë°©ë²•

### 1. ëª¨ë¸ ì •ë³´ í™•ì¸
```bash
python inspect_model.py
```

**ì¶œë ¥ ë‚´ìš©**:
- ëª¨ë¸ ì„¤ì • (ë ˆì´ì–´ ìˆ˜, íŒŒë¼ë¯¸í„° ë“±)
- í•™ìŠµ ìš”ì•½ (Loss, í•™ìŠµ ì‹œê°„ ë“±)
- íŒŒì¼ í¬ê¸°
- ë¡œë“œ í…ŒìŠ¤íŠ¸

### 2. ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
```bash
python test_htp_model.py
```

**í…ŒìŠ¤íŠ¸ í•­ëª©**:
- âœ… ê³ ì • í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 5ê°œ
- âœ… ë°ì´í„°ì…‹ ëœë¤ ìƒ˜í”Œ 10ê°œ
- âœ… ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥ ë¶„ì„
- âœ… ê²°ê³¼ JSON/TXT ì €ì¥

**ì¶œë ¥ íŒŒì¼**:
- `test_results/test_results_YYYYMMDD_HHMMSS.json` (ìƒì„¸ ê²°ê³¼)
- `test_results/test_report_YYYYMMDD_HHMMSS.txt` (ìš”ì•½ ë¦¬í¬íŠ¸)

### 3. ëŒ€í™”í˜• í…ŒìŠ¤íŠ¸
```bash
python interactive_test.py
```

**ì‚¬ìš© ì˜ˆì‹œ**:
```
ğŸ–¼ï¸  ì´ë¯¸ì§€ ìº¡ì…˜ì„ ì…ë ¥í•˜ì„¸ìš”: The tree is very tall with dense branches

ğŸ¤– AI ì‹¬ë¦¬í•™ìê°€ ë¶„ì„ ì¤‘...

ğŸ“‹ ì‹¬ë¦¬í•™ì  í•´ì„:
This suggests a strong, assertive personality with a desire for 
control and leadership. The individual might be perceived as 
confident and possibly even domineering...
```

---

## ğŸ“‚ íŒŒì¼ êµ¬ì¡°

```
models/layer_freezing/
â”œâ”€â”€ final_htp_model.ipynb          # í•™ìŠµ ë…¸íŠ¸ë¶ (ì›ë³¸)
â”œâ”€â”€ qwen2.5-htp-layer-freeze-final/  # í•™ìŠµëœ ëª¨ë¸
â”‚   â”œâ”€â”€ model.safetensors           # ëª¨ë¸ ê°€ì¤‘ì¹˜
â”‚   â”œâ”€â”€ config.json                 # ëª¨ë¸ ì„¤ì •
â”‚   â”œâ”€â”€ tokenizer.json              # í† í¬ë‚˜ì´ì €
â”‚   â”œâ”€â”€ final_training_summary.json # í•™ìŠµ ìš”ì•½
â”‚   â””â”€â”€ training_curves.png         # í•™ìŠµ ê³¡ì„ 
â”œâ”€â”€ test_htp_model.py              # ì „ì²´ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ interactive_test.py            # ëŒ€í™”í˜• í…ŒìŠ¤íŠ¸
â”œâ”€â”€ inspect_model.py               # ëª¨ë¸ ì •ë³´ í™•ì¸
â””â”€â”€ README.md                      # ì´ íŒŒì¼
```

---

## ğŸ¯ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì˜ˆì‹œ

### 1. Tree (ë‚˜ë¬´)
```
ì…ë ¥: "The tree is dominant and tall with many branches"
ì˜ˆìƒ: ìì‹ ê°, ì„±ì¥ ì˜ì§€, ë¦¬ë”ì‹­
```

### 2. House (ì§‘)
```
ì…ë ¥: "A small house with no windows, located far from the center"
ì˜ˆìƒ: ë‚´í–¥ì„±, ì†Œí†µ íšŒí”¼, ê³ ë¦½ê°
```

### 3. Person (ì‚¬ëŒ)
```
ì…ë ¥: "The person is drawn very small and placed at the bottom corner"
ì˜ˆìƒ: ë‚®ì€ ìì¡´ê°, ì†Œì†ê° ë¶€ì¡±
```

---

## âš™ï¸ ìš”êµ¬ì‚¬í•­

### Python ë¼ì´ë¸ŒëŸ¬ë¦¬
```bash
pip install torch transformers datasets numpy matplotlib
```

### í•˜ë“œì›¨ì–´
- **ìµœì†Œ**: 8GB GPU (RTX 4060 ë“±)
- **ê¶Œì¥**: 12GB+ GPU
- **CPU**: ê°€ëŠ¥í•˜ì§€ë§Œ ë§¤ìš° ëŠë¦¼

### ë””ìŠ¤í¬ ê³µê°„
- ëª¨ë¸: ì•½ 3GB
- ì‹¤í–‰ í™˜ê²½: 5GB+ ê¶Œì¥

---

## ğŸ“Š ì„±ëŠ¥ ì§€í‘œ

| í•­ëª© | ê°’ |
|------|-----|
| í•™ìŠµ Loss (ì´ˆê¸°) | 0.841 |
| í•™ìŠµ Loss (ìµœì¢…) | 0.280 |
| Loss ê°œì„ ë¥  | 67% |
| Validation Loss | 1.809 |
| í•™ìŠµ ì‹œê°„ | 39ë¶„ |
| í•™ìŠµ íŒŒë¼ë¯¸í„° ë¹„ìœ¨ | 7-10% |

---

## ğŸ”§ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 1. CUDA Out of Memory
```python
# test_htp_model.pyì—ì„œ max_tokens ì¤„ì´ê¸°
max_tokens=150  # ê¸°ë³¸ê°’ 256ì—ì„œ ì¤„ì„
```

### 2. ë°˜ë³µì ì¸ ì¶œë ¥ ("system system...")
```python
# ì´ë¯¸ í•´ê²°ë¨! 
# repetition_penalty=1.2
# no_repeat_ngram_size=3
```

### 3. ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨
```bash
# ê²½ë¡œ í™•ì¸
ls ./qwen2.5-htp-layer-freeze-final/

# ê¶Œí•œ í™•ì¸
chmod -R 755 ./qwen2.5-htp-layer-freeze-final/
```

---

## ğŸ“ ì£¼ìš” í•¨ìˆ˜

### generate_htp_interpretation()
```python
def generate_htp_interpretation(instruction, image_caption, max_tokens=256):
    """
    HTP ì´ë¯¸ì§€ ìº¡ì…˜ â†’ ì‹¬ë¦¬í•™ì  í•´ì„ ìƒì„±
    
    Args:
        instruction: í•´ì„ ìš”ì²­ ì§€ì‹œë¬¸
        image_caption: ì´ë¯¸ì§€ ì„¤ëª… (ì˜ˆ: "The tree is tall")
        max_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
    
    Returns:
        str: ì‹¬ë¦¬í•™ì  í•´ì„ í…ìŠ¤íŠ¸
    """
```

### ìƒì„± íŒŒë¼ë¯¸í„°
```python
temperature=0.8        # ë‹¤ì–‘ì„± (ë†’ì„ìˆ˜ë¡ ì°½ì˜ì )
top_p=0.95            # Nucleus sampling
top_k=40              # Top-K sampling
repetition_penalty=1.2  # ë°˜ë³µ ë°©ì§€
no_repeat_ngram_size=3  # 3-gram ë°˜ë³µ ë°©ì§€
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

### HTP ê²€ì‚¬ë€?
- **H**ouse-**T**ree-**P**erson Test
- íˆ¬ì‚¬ì  ì‹¬ë¦¬ê²€ì‚¬ ê¸°ë²•
- ê·¸ë¦¼ì„ í†µí•œ ë¬´ì˜ì‹ ë¶„ì„

### ëª¨ë¸ ì•„í‚¤í…ì²˜
- Qwen2.5: Alibabaì˜ ìµœì‹  ì–¸ì–´ ëª¨ë¸
- Instruction Tuning: ì§€ì‹œë¬¸ ë”°ë¥´ê¸° í•™ìŠµ
- Layer Freezing: íš¨ìœ¨ì  íŒŒì¸íŠœë‹

---

## ğŸ“§ ë¬¸ì˜

ë¬¸ì œê°€ ë°œìƒí•˜ë©´ ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:
1. GPU ë©”ëª¨ë¦¬ ì¶©ë¶„í•œê°€?
2. ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ë˜ì—ˆë‚˜?
3. ëª¨ë¸ íŒŒì¼ì´ ëª¨ë‘ ì¡´ì¬í•˜ë‚˜?

---

**ë§Œë“  ë‚ ì§œ**: 2025-11-15  
**ë²„ì „**: 1.0.0  
**ë¼ì´ì„¼ìŠ¤**: MIT
