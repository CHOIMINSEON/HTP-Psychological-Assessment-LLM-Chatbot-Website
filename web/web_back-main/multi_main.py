from dotenv import load_dotenv
load_dotenv()
from embeddings import vectorstore           # ë²¡í„° DB
from rag_engine import AdvancedConversationalRAG  # ë©€í‹°ì¿¼ë¦¬ RAG ì—”ì§„

from fastapi import FastAPI
from pydantic import BaseModel
from typing import Optional, Dict, Any
from caption import generate_caption
from model import generate_with_qwen
from fastapi.middleware.cors import CORSMiddleware
import logging
import json

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

app = FastAPI()
# CORS ì„¤ì • - ë” ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ëª¨ë“  ì¶œì²˜ í—ˆìš©
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
    expose_headers=["*"],
    max_age=3600,
)

# -------------------------------------
# RAG ì—”ì§„ ì´ˆê¸°í™”
# -------------------------------------
rag = AdvancedConversationalRAG(vectorstore)

# ----------------------------- #
# 1) ì´ë¯¸ì§€ ìº¡ì…˜ ìƒì„±
# ----------------------------- #
class CaptionRequest(BaseModel):
    image_base64: str

@app.post("/caption")
def caption(req: CaptionRequest):
    logger.info("=" * 80)
    logger.info("ğŸ“¸ [CAPTION] ì´ë¯¸ì§€ ìº¡ì…˜ ìƒì„± ì‹œì‘")
    logger.info("ğŸ¤– ì‚¬ìš© ëª¨ë¸: Florence-2-large")
    logger.info(f"ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°: {len(req.image_base64)} bytes")
    
    caption = generate_caption(req.image_base64)
    
    logger.info(f"âœ… [CAPTION] ìƒì„±ëœ ìº¡ì…˜: {caption}")
    logger.info("=" * 80)
    return {"caption": caption}

# ----------------------------- #
# 2) ë©€í‹°ì¿¼ë¦¬ ê¸°ë°˜ RAG ê²€ìƒ‰
# ----------------------------- #
class RagRequest(BaseModel):
    caption: str
    image_type: str    # "ì§‘" | "ë‚˜ë¬´" | "ì‚¬ëŒ"

@app.post("/rag")
def rag_search_api(req: RagRequest):
    logger.info("=" * 80)
    logger.info("ğŸ” [RAG] RAG ê²€ìƒ‰ ì‹œì‘ (ê²€ìƒ‰ ì „ìš© ëª¨ë“œ)")
    logger.info("ğŸ¤– ì¿¼ë¦¬ ì¬ì‘ì„± ëª¨ë¸: GPT-4o (OpenAI)")
    logger.info(f"ì…ë ¥ ìº¡ì…˜: {req.caption}")
    logger.info(f"ì´ë¯¸ì§€ íƒ€ì…: {req.image_type}")
    
    try:
        # search_only ë©”ì„œë“œ ì‚¬ìš© (í•´ì„ ìƒì„± ì œê±°)
        result = rag.search_only(req.caption, req.image_type)
        
        logger.info(f"âœ… [RAG] ê²€ìƒ‰ ì™„ë£Œ")
        logger.info(f"ì¬ì‘ì„±ëœ ì¿¼ë¦¬: {result.get('rewritten_queries', [])}")
        logger.info(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(result.get('rag_docs', []))}")
        
        # ê° ë¬¸ì„œì˜ ë‚´ìš© ì¶œë ¥
        for idx, doc in enumerate(result.get('rag_docs', []), 1):
            logger.info(f"\nğŸ“„ ë¬¸ì„œ {idx}:")
            logger.info(f"  ë‚´ìš©: {doc[:200]}..." if len(doc) > 200 else f"  ë‚´ìš©: {doc}")
        
        logger.info("=" * 80)
        return result
        
    except Exception as e:
        logger.error(f"âŒ [RAG] ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
        logger.error(f"ì—ëŸ¬ íƒ€ì…: {type(e).__name__}")
        import traceback
        logger.error(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")
        logger.info("=" * 80)
        
        # ë¹ˆ ê²°ê³¼ ë°˜í™˜ (ì—ëŸ¬ ë°œìƒ ì‹œ)
        return {
            "rewritten_queries": [req.caption],
            "rag_docs": [],
            "error": str(e)
        }

# ----------------------------- #
# 3) Qwen ë¡œë¼ ëª¨ë¸ ê°œë³„ í•´ì„
# ----------------------------- #
class InterpretSingle(BaseModel):
    caption: str
    rag_docs: list
    image_type: str

@app.post("/interpret_single")
def interpret_single(req: InterpretSingle):
    logger.info("=" * 80)
    logger.info("ğŸ§  [INTERPRET_SINGLE] ê°œë³„ í•´ì„ ì‹œì‘")
    logger.info("ğŸ¤– ì‚¬ìš© ëª¨ë¸: Qwen (helena29/Qwen2.5_LoRA_for_HTP)")
    logger.info(f"ì´ë¯¸ì§€ íƒ€ì…: {req.image_type}")
    logger.info(f"ì…ë ¥ ìº¡ì…˜: {req.caption}")
    logger.info(f"RAG ë¬¸ì„œ ìˆ˜: {len(req.rag_docs)}")
    
    # RAG ë¬¸ì„œê°€ ìˆìœ¼ë©´ ì°¸ê³ ë¬¸í—Œìœ¼ë¡œ í™œìš©
    reference_context = ""
    if req.rag_docs and len(req.rag_docs) > 0:
        # RAG ë¬¸ì„œê°€ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
        if isinstance(req.rag_docs, list):
            # ê° ë¬¸ì„œë¥¼ ìš”ì•½í•´ì„œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            ref_docs = "\n".join([f"- {str(doc)[:300]}" for doc in req.rag_docs[:3]])  # ìµœëŒ€ 3ê°œ ë¬¸ì„œ, ê° 300ì
            reference_context = f"\n\nReference Literature:\n{ref_docs}"
            logger.info(f"âœ… RAG ë¬¸ì„œ {len(req.rag_docs[:3])}ê°œë¥¼ ì°¸ê³ í•˜ì—¬ í•´ì„")
        else:
            logger.warning(f"âš ï¸  RAG ë¬¸ì„œ í˜•ì‹ ì˜¤ë¥˜: {type(req.rag_docs)}")
    else:
        logger.info("âš ï¸  RAG ë¬¸ì„œ ì—†ìŒ - ì¼ë°˜ì ì¸ HTP ì›ë¦¬ë¡œ í•´ì„")
    
    # ëª¨ë¸ì˜ fine-tuning í˜•ì‹ì— ë§ì¶˜ í”„ë¡¬í”„íŠ¸ êµ¬ì¡°
    # instructionê³¼ inputì„ ëª…í™•íˆ ë¶„ë¦¬
    result = generate_with_qwen(caption=req.caption, context=reference_context)
    
    logger.info(f"âœ… [INTERPRET_SINGLE] í•´ì„ ì™„ë£Œ")
    logger.info(f"ìƒì„±ëœ í•´ì„: {result[:200]}..." if len(result) > 200 else f"ìƒì„±ëœ í•´ì„: {result}")
    logger.info("=" * 80)
    return {"interpretation": result}

# ----------------------------- #
# 4) GPT ë²ˆì—­ API
# ----------------------------- #
from openai import OpenAI
client = OpenAI()

class TranslateRequest(BaseModel):
    text: str

@app.post("/translate")
def translate(req: TranslateRequest):
    """ì˜ì–´ í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­"""
    logger.info("ğŸŒ [TRANSLATE] ë²ˆì—­ ì‹œì‘")
    logger.info(f"ì›ë¬¸ (ì˜ì–´): {req.text[:100]}...")
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are a professional translator. Translate the given English text to natural Korean. Only provide the translation, nothing else."},
                {"role": "user", "content": req.text}
            ],
            temperature=0.3
        )
        
        translated = response.choices[0].message.content
        logger.info(f"ë²ˆì—­ ê²°ê³¼ (í•œêµ­ì–´): {translated[:100]}...")
        return {"translated": translated}
        
    except Exception as e:
        logger.error(f"âŒ [TRANSLATE] ë²ˆì—­ ì‹¤íŒ¨: {str(e)}")
        return {"translated": req.text}  # ì‹¤íŒ¨ì‹œ ì›ë¬¸ ë°˜í™˜

# ----------------------------- #
# 5) Qwen ëª¨ë¸ë¡œ ì¶”ê°€ ì§ˆë¬¸ ìƒì„± (ì˜ì–´)
# ----------------------------- #

class QuestionReq(BaseModel):
    conversation: list
    interpretations: Optional[Dict[str, Any]] = None  # { house: str, tree: str, person: str }

@app.post("/questions")
def questions(req: QuestionReq):
    logger.info("=" * 80)
    logger.info("â“ [QUESTIONS] ì¶”ê°€ ì§ˆë¬¸ ìƒì„± ì‹œì‘")
    logger.info("ğŸ¤– ì‚¬ìš© ëª¨ë¸: (ê³ ì •í˜• ì§ˆë¬¸ ëª¨ë“œ - LLM í˜¸ì¶œ ì—†ìŒ)")
    logger.info(f"ëŒ€í™” ê¸°ë¡ ìˆ˜: {len(req.conversation)}")
    
    for idx, msg in enumerate(req.conversation[-3:], 1):  # ìµœê·¼ 3ê°œë§Œ ë¡œê¹…
        logger.info(f"  ë©”ì‹œì§€ {idx}: {msg.get('role')} - {msg.get('content')[:100]}...")
    
    # ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜
    conversation_text = "\n".join([
        f"{msg.get('role').upper()}: {msg.get('content')}" 
        for msg in req.conversation
    ])

    # í•´ì„ ì»¨í…ìŠ¤íŠ¸(ìˆëŠ” ê²½ìš°) í¬í•¨
    interp_text = ""
    if req.interpretations:
        house = req.interpretations.get("house", "")
        tree = req.interpretations.get("tree", "")
        person = req.interpretations.get("person", "")
        # ê° í•´ì„ì„ ìš”ì•½í•´ì„œ í¬í•¨ (ë„ˆë¬´ ê¸¸ë©´ ëª¨ë¸ ì„±ëŠ¥ ì €í•˜)
        house_summary = house[:200] + "..." if len(house) > 200 else house
        tree_summary = tree[:200] + "..." if len(tree) > 200 else tree
        person_summary = person[:200] + "..." if len(person) > 200 else person
        
        interp_text = f"""
Drawing Interpretations:
- House: {house_summary}
- Tree: {tree_summary}
- Person: {person_summary}
"""

    # ëª¨ë¸ì˜ fine-tuning í˜•ì‹ì— ë§ì¶˜ ê°„ê²°í•œ í”„ë¡¬í”„íŠ¸
    # ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ ìˆìœ¼ë©´ ì´ë¥¼ ìš°ì„  ê³ ë ¤, ì—†ìœ¼ë©´ í•´ì„ë§Œ ì‚¬ìš©
    if conversation_text.strip():
        context_section = f"Previous Conversation:\n{conversation_text}\n{interp_text}"
    else:
        context_section = f"Drawing Analysis:{interp_text}"
    
    # -----------------------------
    # ê³ ì •í˜• 5ê°œ ì§ˆë¬¸ ìˆœì°¨ ë°˜í™˜ ë¡œì§
    # -----------------------------
    FIXED_QUESTIONS = [
    "ê·¸ë¦¼ì„ ê·¸ë¦´ ë•Œ ì „ë°˜ì ì¸ ê¸°ë¶„ì´ë‚˜ ë§ˆìŒê°€ì§ì€ ì–´ë– ì…¨ë‚˜ìš”?",
    "ê·¸ë¦¬ëŠ” ë™ì•ˆ íŠ¹ë³„íˆ ì‹ ê²½ ì¨ì„œ ê·¸ë ¸ê±°ë‚˜, ë°˜ëŒ€ë¡œ ê·¸ë¦¬ê¸° ë§ì„¤ì—¬ì¡Œë˜ ë¶€ë¶„ì´ ìˆì—ˆë‚˜ìš”?",
    "ê·¸ë¦¼ì„ ì™„ì„±í•œ ì§í›„, ê°€ì¥ ë¨¼ì € ë“  ìƒê°ì´ë‚˜ ëŠë‚Œì€ ë¬´ì—‡ì´ì—ˆë‚˜ìš”?",
    "ê·¸ë¦¬ëŠ” ê³¼ì •ì—ì„œ ê°ì •ì´ë‚˜ ê¸°ë¶„ì˜ ë³€í™”ê°€ ëŠê»´ì§€ì…¨ë‚˜ìš”?",
    "ì´ ê·¸ë¦¼ì— ëŒ€í•´ ë§ë¶™ì—¬ ì„¤ëª…í•˜ê±°ë‚˜ í•˜ê³  ì‹¶ì€ ì´ì•¼ê¸°ê°€ ìˆë‚˜ìš”?"
]

    # assistant ë©”ì‹œì§€ ìˆ˜ë¡œ í˜„ì¬ ì§ˆë¬¸ ë²ˆí˜¸ ê²°ì • (0ë¶€í„° ì‹œì‘)
    assistant_count = sum(1 for m in req.conversation if m.get("role") == "assistant")
    question_index = assistant_count % len(FIXED_QUESTIONS)
    
    next_q = FIXED_QUESTIONS[question_index]

    logger.info(f"ğŸ§© ì§ˆë¬¸ ë²ˆí˜¸: {question_index + 1}/{len(FIXED_QUESTIONS)}")
    logger.info(f"âœ… [QUESTIONS] ìµœì¢… ì§ˆë¬¸: {next_q}")
    logger.info("=" * 80)
    return {"question": next_q}

# ----------------------------- #
# 6) ìµœì¢… í•´ì„ (GPT-4o)
# ----------------------------- #
class InterpretFinal(BaseModel):
    single_results: dict
    conversation: list
    user_info: Optional[Dict[str, Any]] = None  # { name: str, age: str/int, gender: 'male'|'female' }

@app.post("/interpret_final")
def interpret_final(req: InterpretFinal):
    logger.info("=" * 80)
    logger.info("ğŸ¯ [INTERPRET_FINAL] ìµœì¢… í•´ì„ ìƒì„± ì‹œì‘")
    logger.info("ğŸ¤– ì‚¬ìš© ëª¨ë¸: GPT-4o (OpenAI)")
    logger.info(f"ì§‘ í•´ì„: {req.single_results.get('house', 'ì—†ìŒ')[:100]}...")
    logger.info(f"ë‚˜ë¬´ í•´ì„: {req.single_results.get('tree', 'ì—†ìŒ')[:100]}...")
    logger.info(f"ì‚¬ëŒ í•´ì„: {req.single_results.get('person', 'ì—†ìŒ')[:100]}...")
    logger.info(f"ëŒ€í™” ê¸°ë¡ ìˆ˜: {len(req.conversation)}")
    
    # ì‚¬ìš©ì ì •ë³´ ë°˜ì˜
    name = None
    age = None
    gender = None
    if req.user_info:
        name = req.user_info.get('name')
        age = req.user_info.get('age')
        gender = req.user_info.get('gender')

    # ì„±ë³„/ë‚˜ì´ ì„¤ëª… ë¬¸êµ¬ êµ¬ì„± (í•´ì„ ì°¸ê³ ìš©)
    demo_context_lines = []
    if age:
        demo_context_lines.append(f"- ê²€ì‚¬ìì˜ ë‚˜ì´: {age} (ë°œë‹¬ ë‹¨ê³„ ë° ì—°ë ¹ íŠ¹ì„±ì„ ê³ ë ¤í•´ í•´ì„ì— ì°¸ê³ )\n")
    if gender:
        ko_gender = 'ì—¬ì„±' if str(gender).lower() == 'female' else ('ë‚¨ì„±' if str(gender).lower() == 'male' else str(gender))
        demo_context_lines.append(f"- ê²€ì‚¬ìì˜ ì„±ë³„: {ko_gender} (ì„±ë³„ì— ë”°ë¥¸ ì¼ë°˜ì  ê²½í–¥ì„ ì°¸ê³ í•˜ë˜, ê³ ì •ê´€ë…ì€ í”¼í•  ê²ƒ)\n")
    demo_context = "".join(demo_context_lines)

    # GPT ë©”ì‹œì§€ êµ¬ì„±
    messages = [
        {
            "role": "system",
            "content": "You are a professional psychological counselor specializing in HTP (House-Tree-Person) test interpretation. Provide comprehensive, insightful psychological analysis in Korean."
        },
        {
            "role": "user",
            "content": f"""
ë‹¹ì‹ ì€ ì „ë¬¸ ì‹¬ë¦¬ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ì•„ë˜ HTP ê²€ì‚¬ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ì‹¬ë¦¬ í•´ì„ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

ì§‘ í•´ì„ (House Interpretation):
{req.single_results.get('house','N/A')}

ë‚˜ë¬´ í•´ì„ (Tree Interpretation):
{req.single_results.get('tree','N/A')}

ì‚¬ëŒ í•´ì„ (Person Interpretation):
{req.single_results.get('person','N/A')}

ì‚¬ìš©ìì™€ ë‚˜ëˆˆ ëŒ€í™”:
{req.conversation}

ê²€ì‚¬ì ì •ë³´:
{('- ì´ë¦„: ' + str(name) + '\n') if name else ''}{demo_context}

ìœ„ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… HTP ì‹¬ë¦¬ í•´ì„ì„ 5ê°œ ë¬¸ë‹¨ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.

ì¤‘ìš” ì§€ì¹¨:
- ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”
- ê° ê·¸ë¦¼(ì§‘, ë‚˜ë¬´, ì‚¬ëŒ)ì˜ ê°œë³„ í•´ì„ì„ í†µí•©í•˜ì—¬ ì „ì²´ì ì¸ ì‹¬ë¦¬ ìƒíƒœë¥¼ ë¶„ì„í•˜ì„¸ìš”
- ì‚¬ìš©ìì™€ì˜ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ë” ê¹Šì´ ìˆëŠ” í•´ì„ì„ ì œê³µí•˜ì„¸ìš”
- ì „ë¬¸ì ì´ê³  ë”°ëœ»í•œ ì–´ì¡°ë¡œ ì‘ì„±í•˜ì„¸ìš”
- 5ê°œ ë¬¸ë‹¨ìœ¼ë¡œ êµ¬ì„±í•˜ì„¸ìš”
"""
        }
    ]
    
    logger.info(f"ğŸ“ GPT ìš”ì²­ ì „ì†¡ ì¤‘...")
    
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=messages,
        temperature=0.7,
        max_tokens=2000
    )
    
    result = response.choices[0].message.content
    
    logger.info(f"âœ… [INTERPRET_FINAL] ìµœì¢… í•´ì„ ì™„ë£Œ")
    logger.info(f"ìƒì„±ëœ ìµœì¢… í•´ì„ (ì²˜ìŒ 200ì): {result[:200]}...")
    logger.info("=" * 80)
    return {"final": result}
