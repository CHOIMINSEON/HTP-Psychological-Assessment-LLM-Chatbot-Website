from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# ì „ì—­ ë³€ìˆ˜ë¡œ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ í•œ ë²ˆë§Œ ë¡œë“œ
_model = None
_tokenizer = None
_model_name = "helena29/Qwen2.5_LoRA_for_HTP"

def _load_model():
    """ëª¨ë¸ì„ í•œ ë²ˆë§Œ ë¡œë“œ (ì‹±ê¸€í†¤ íŒ¨í„´)"""
    global _model, _tokenizer
    
    if _model is None:
        print(f"ğŸ”¥ Loading Qwen HTP Model: {_model_name}")
        print(f"ğŸ” CUDA Available: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"ğŸ” CUDA Device: {torch.cuda.get_device_name(0)}")
            print(f"ğŸ” CUDA Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        _tokenizer = AutoTokenizer.from_pretrained(_model_name)
        
        # ëª¨ë¸ ë¡œë“œ (LoRA ì–´ëŒ‘í„°ê°€ ì´ë¯¸ ë³‘í•©ëœ ìƒíƒœ)
        _model = AutoModelForCausalLM.from_pretrained(
            _model_name,
            device_map="auto",
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
        )
        
        print(f"âœ… Qwen HTP Model loaded successfully!")
        print(f"âœ… Model Device: {_model.device}")
    
    return _model, _tokenizer


def _clean_output(text: str) -> str:
    """
    ëª¨ë¸ ì¶œë ¥ í›„ì²˜ë¦¬: ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±° ë° ë¶ˆì™„ì „í•œ ë¬¸ì¥ ì²˜ë¦¬
    """
    import re
    
    # ë”°ì˜´í‘œë‚˜ ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
    text = text.strip('`"\'').strip()
    
    # "Output:", "Answer:", "Response:" ê°™ì€ í”„ë¦¬í”½ìŠ¤ ì œê±°
    text = re.sub(r'^(Output|Answer|Response|Result):\s*', '', text, flags=re.IGNORECASE)
    
    # ì—°ì†ëœ ê³µë°±ì´ë‚˜ ì¤„ë°”ê¿ˆ ì •ë¦¬
    text = re.sub(r'\n{3,}', '\n\n', text)
    text = re.sub(r' {2,}', ' ', text)
    
    # ë¶ˆì™„ì „í•œ ë¬¸ì¥ ê°ì§€ ë° ì œê±°
    text = text.strip()
    if text and text[-1] not in '.!?ã€‚':
        # ë§ˆì§€ë§‰ ì™„ì „í•œ ë¬¸ì¥ ë¶€í˜¸ ì°¾ê¸°
        last_complete_idx = -1
        for i in range(len(text) - 1, -1, -1):
            if text[i] in '.!?ã€‚':
                last_complete_idx = i
                break
        
        # ì™„ì „í•œ ë¬¸ì¥ì´ ìˆìœ¼ë©´ ê±°ê¸°ê¹Œì§€ë§Œ ìœ ì§€
        if last_complete_idx > 0:
            text = text[:last_complete_idx + 1]
    
    return text.strip()


def generate_with_qwen(caption: str, context: str = ""):
    """
    Qwen ëª¨ë¸ì„ ì‚¬ìš©í•´ HTP í•´ì„ ìƒì„± (Chat Template + Prefill ì ìš©)
    """
    model, tokenizer = _load_model()
    
    # ------------------------------------------------------------------
    # [ìˆ˜ì • 1] ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ ìœ ì € ì…ë ¥ì„ ë¶„ë¦¬í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ì •ì˜
    # ------------------------------------------------------------------
    system_prompt = """You are an expert in HTP (House-Tree-Person) projective drawing analysis. 
Analyze the provided "Drawing Observations" based on standard psychological theories.

### Constraints
1. Disclaimer: Educational purpose only. Not a medical diagnosis.
2. Format: Strictly follow the output format.
3. Tone: Analytical, objective, and empathetic.
4. Stop: Do NOT generate conversational fillers (e.g., "Here is the analysis").

### Response Format
1. Feature Analysis:
   - [Feature Name]: [Meaning]

2. Psychological Synthesis:
   [Summary]"""

    user_content = f"Drawing Observations: {caption}{context}\n\nAnalyze this strictly based on the format."

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_content}
    ]

    # ------------------------------------------------------------------
    # [ìˆ˜ì • 2] apply_chat_template ì‚¬ìš© (ëª¨ë¸ì´ ì´í•´í•˜ëŠ” í¬ë§·ìœ¼ë¡œ ë³€í™˜)
    # ------------------------------------------------------------------
    text_input = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    # ------------------------------------------------------------------
    # [ìˆ˜ì • 3] Assistant Prefill (ë‹µë³€ ê°•ì œ ì‹œì‘) - í•µì‹¬!
    # ëª¨ë¸ì´ ë”´ì†Œë¦¬ ëª»í•˜ê²Œ ì•„ì˜ˆ ì²« ì¤„ì„ ìš°ë¦¬ê°€ ì ì–´ì¤ë‹ˆë‹¤.
    # ------------------------------------------------------------------
    text_input += "1. Feature Analysis:\n"

    print("=" * 80)
    print(f"ğŸ“ [PROMPT] ìµœì¢… ì…ë ¥ í”„ë¡¬í”„íŠ¸:\n{text_input}")
    print("=" * 80)

    inputs = tokenizer([text_input], return_tensors="pt").to(model.device)

    # ------------------------------------------------------------------
    # [ìˆ˜ì • 4] ìƒì„± íŒŒë¼ë¯¸í„° ìµœì í™”
    # ------------------------------------------------------------------
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=512,      # ìµœëŒ€ ê¸¸ì´
            # min_new_tokens=150,    # [ì‚­ì œ] ì–µì§€ë¡œ ê¸¸ê²Œ ì“°ë ¤ë‹¤ í• ë§ ì—†ìœ¼ë©´ ì†Œì„¤ ì”ë‹ˆë‹¤.
            temperature=0.1,         # [ìˆ˜ì •] 0.65 -> 0.1 (ì¼ê´€ì„± ìœ„í•´ ë§¤ìš° ë‚®ê²Œ ì„¤ì •)
            top_p=0.9,
            do_sample=True,          # Falseë¡œ í•´ë„ ë˜ì§€ë§Œ, 0.1ì´ë©´ Trueë„ ì•ˆì „í•¨
            repetition_penalty=1.1,  # 1.15 -> 1.1 (ë„ˆë¬´ ë†’ìœ¼ë©´ ë¬¸ë²• ê¹¨ì§)
            
            # [ìˆ˜ì • 5] Stop Token ì„¤ì • (ì´ìƒí•œ í„´ ìƒì„± ë°©ì§€)
            stop_strings=["Human:", "User:", "###", "Drawing Observations:"],
            tokenizer=tokenizer      # stop_strings ì‚¬ìš© ì‹œ í•„ìš”í•  ìˆ˜ ìˆìŒ
        )

    # ------------------------------------------------------------------
    # [ìˆ˜ì • 6] ê²°ê³¼ í›„ì²˜ë¦¬ (Prefill í–ˆë˜ ë¶€ë¶„ ë‹¤ì‹œ ë¶™ì—¬ì£¼ê¸°)
    # ------------------------------------------------------------------
    # ì…ë ¥ í† í° ê¸¸ì´ë§Œí¼ ì˜ë¼ëƒ„
    input_len = inputs["input_ids"].shape[1]
    generated_ids = outputs[0][input_len:]
    decoded_output = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()
    
    # ìš°ë¦¬ê°€ ê°•ì œë¡œ ë„£ì—ˆë˜ "1. Feature Analysis:\n"ê°€ ì¶œë ¥ì—” ë¹ ì ¸ìˆìœ¼ë¯€ë¡œ ë‹¤ì‹œ ë¶™ì„
    final_result = "1. Feature Analysis:\n" + decoded_output
    
    # í˜¹ì‹œ ëª¨ë¥¼ ë’·ë¶€ë¶„ ì¡ë™ì‚¬ë‹ˆ ì œê±° (2. ì‹¬ë¦¬ì  ì¢…í•© ë’·ë¶€ë¶„ ìë¥´ê¸°)
    if "2. Psychological Synthesis:" in final_result:
        # ì„¹ì…˜ 2ê°€ ì‹œì‘ëœ í›„, ì¤„ë°”ê¿ˆì´ 3ë²ˆ ì´ìƒ ë‚˜ì˜¤ë©´ ê·¸ ë’¤ëŠ” ìë¦„ (Stop token ì‹¤íŒ¨ ëŒ€ë¹„)
        parts = final_result.split("2. Psychological Synthesis:")
        synthesis_part = parts[1]
        # ê°„ë‹¨í•œ íŒŒì‹± ë¡œì§: ë¬¸ë‹¨ì´ ëë‚˜ê³  ë‹¤ë¥¸ í—¤ë”ê°€ ë‚˜ì˜¤ê±°ë‚˜ ë„ˆë¬´ ê¸¸ì–´ì§€ë©´ ìë¦„
        # (ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœí•˜ê²Œ ìœ ì§€)
        pass

    result = _clean_output(final_result)
    
    print(f"âœ… [Result] Generated length: {len(result)}")
    return result